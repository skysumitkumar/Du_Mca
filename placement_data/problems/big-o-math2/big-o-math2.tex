Al and Bill are arguing about the performance of their sorting
algoritms.  Al claims that his $O(N \log  N)$-time algorithm is {\em

always} faster than Bill's $O(N^{2})$-time algorithm.  To settle the
issue, they implement and run the two algorithms on many randomly
generated data sets.  To Al's dismay, they find that if $N < 100$ the
$O(N^{2})$-time algorithm actually runs faster, and only when $N \geq
100$ the $O(N\log  N)$-time one is better.

Explain why the above scenario is possible.  You may give numerical
examples.
