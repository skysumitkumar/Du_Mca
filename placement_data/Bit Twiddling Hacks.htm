<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0051)http://graphics.stanford.edu/~seander/bithacks.html -->
<HTML><HEAD><TITLE>Bit Twiddling Hacks</TITLE>
<META http-equiv=Content-Type content="text/html; charset=utf-8">
<META content="MSHTML 6.00.2800.1106" name=GENERATOR></HEAD>
<BODY>
<H2>Bit Twiddling Hacks </H2>
<H3>By Sean Eron Anderson<BR>seander@cs.<!-- NO SPAM! -->stanford.edu 
</H3><SMALL>Individually, the <STRONG>code snippets here are in the public 
domain</STRONG> (unless otherwise noted) — feel free to use them however you 
please. The aggregate collection and descriptions are ©&nbsp;1997-2005 Sean Eron 
Anderson. The code and descriptions are distributed in the hope that they will 
be useful, but <STRONG>WITHOUT ANY WARRANTY</STRONG> and without even the 
implied warranty of merchantability or fitness for a particular purpose. As of 
May 5, 2005, all the code has been tested thoroughly. Thousands of people have 
read it. Moreover, <A href="http://www-2.cs.cmu.edu/~bryant/">Professor Randal 
Bryant</A>, the Dean of Computer Science at Carnegie Mellon University, has 
personally tested almost everything with his <A 
href="http://www-2.cs.cmu.edu/~uclid/">Uclid code verification system</A>. What 
he hasn't tested, I have checked against all possible (32-bit) inputs. 
<STRONG>To the first person to inform me of a legitimate bug in the code, I'll 
pay a bounty of US$10 (by check or Paypal)</STRONG>.</SMALL> 
<P>
<P>
<H3>Contents </H3>
<UL>
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#OperationCounting">About 
  the operation counting methodology</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#CopyIntegerSign">Compute 
  the sign of an integer</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerAbs">Compute 
  the integer absolute value (abs) without branching</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerMinOrMax">Compute 
  the minimum (min) or maximum (max) of two integers without branching</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#DetermineIfPowerOf2">Determining 
  if an integer is a power of 2</A> 
  <LI>Sign extending 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#FixedSignExtend">Sign 
    extending from a constant bit width</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#VariableSignExtend">Sign 
    extending from a variable bit-width</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#VariableSignExtendRisky">Sign 
    extending from a variable bit-width in 3 operations</A> </LI></UL>
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#ConditionalSetOrClearBitsWithoutBranching">Conditionally 
  set or clear bits without branching</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#MaskedMerge">Merge 
  bits from two values according to a mask</A> 
  <LI>Counting bits set 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetNaive">Counting 
    bits set, naive way</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetTable">Counting 
    bits set by lookup table</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetKernighan">Counting 
    bits set, Brian Kernighan's way</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSet64">Counting 
    bits set in 12, 24, or 32-bit words using 64-bit instructions</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel">Counting 
    bits set, in parallel</A> </LI></UL>
  <LI>Computing parity (1 if an odd number of bits set, 0 otherwise) 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ParityNaive">Compute 
    parity of a word the naive way</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ParityLookupTable">Compute 
    parity by lookup table</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ParityWith64Bits">Compute 
    parity of a byte using 64-bit multiply and modulus division</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ParityParallel">Compute 
    parity in parallel</A> </LI></UL>
  <LI>Swapping Values 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#SwappingValuesSubAdd">Swapping 
    values with subtraction and addition</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#SwappingValuesXOR">Swapping 
    values with XOR</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#SwappingBitsXOR">Swapping 
    individual bits with XOR</A> </LI></UL>
  <LI>Reversing bit sequences 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#BitReverseObvious">Reverse 
    bits the obvious way</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#BitReverseTable">Reverse 
    bits in word by lookup table</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith64BitsDiv">Reverse 
    the bits in a byte with 3 operations (64-bit muliply and modulus 
    division)</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith64Bits">Reverse 
    the bits in a byte with 4 operations (64-bit multiply, no division)</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith32Bits">Reverse 
    the bits in a byte with 7 operations (no 64-bit, only 32)</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ReverseParallel">Reverse 
    an N-bit quantity in parallel with 5 * lg(N) operations</A> </LI></UL>
  <LI>Modulus division (aka computing <EM>remainders</EM>) 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ModulusDivisionEasy">Computing 
    modulus division by 1 &lt;&lt; s without a division operation (obvious)</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ModulusDivision">Computing 
    modulus division by (1 &lt;&lt; s) - 1 without a division operation</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ModulusDivisionParallel">Computing 
    modulus division by (1 &lt;&lt; s) - 1 in parallel without a division 
    operation</A> </LI></UL>
  <LI>Finding integer log base 2 of an integer (aka the position of the highest 
  bit set) 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious">Find 
    the log base 2 of an integer with the MSB N set in O(N) operations (the 
    obvious way)</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogIEEE64Float">Find 
    the integer log base 2 of an integer with an 64-bit IEEE float</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogLookup">Find 
    the log base 2 of an integer with a lookup table</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLog">Find 
    the log base 2 of an N-bit integer in O(lg(N)) operations</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogDeBruijn">Find 
    the log base 2 of an N-bit integer in O(lg(N)) operations with multiply and 
    lookup</A> </LI></UL>
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLog10">Find 
  integer log base 10 of an integer</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLog10Obvious">Find 
  integer log base 10 of an integer the obvious way</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogFloat">Find 
  integer log base 2 of a 32-bit IEEE float</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogRootFloat">Find 
  integer log base 2 of the pow(2, r)-root of a 32-bit IEEE float (for unsigned 
  integer r)</A> 
  <LI>Counting consecutive trailing zero bits (or finding bit indices) 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ZerosOnRightLinear">Count 
    the consecutive zero bits (trailing) on the right linearly</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ZerosOnRightParallel">Count 
    the consecutive zero bits (trailing) on the right in parallel</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ZerosOnRightBinSearch">Count 
    the consecutive zero bits (trailing) on the right by binary search</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ZerosOnRightFloatCast">Count 
    the consecutive zero bits (trailing) on the right by casting to a float</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ZerosOnRightModLookup">Count 
    the consecutive zero bits (trailing) on the right with modulus division and 
    lookup</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ZerosOnRightMultLookup">Count 
    the consecutive zero bits (trailing) on the right with multiply and 
    lookup</A> </LI></UL>
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2Float">Round 
  up to the next highest power of 2 by float casting</A> 
  <LI><A 
  href="http://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2">Round 
  up to the next highest power of 2</A> 
  <LI>Interleaving bits (aka computing <EM>Morton Numbers</EM>) 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#InterleaveTableObvious">Interleave 
    bits the obvious way</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#InterleaveTableLookup">Interleave 
    bits by table lookup</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#Interleave64bitOps">Interleave 
    bits with 64-bit multiply</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#InterleaveBMN">Interleave 
    bits by Binary Magic Numbers</A> </LI></UL>
  <LI>Testing for ranges of bytes in a word (and counting occurances found) 
  <UL>
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#ZeroInWord">Determine 
    if a word has a zero byte</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#HasLessInWord">Determine 
    if a word has byte less than n</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#HasMoreInWord">Determine 
    if a word has a byte greater than n</A> 
    <LI><A 
    href="http://graphics.stanford.edu/~seander/bithacks.html#HasBetweenInWord">Determine 
    if a word has a byte between m and n</A> </LI></UL></LI></UL>
<HR>

<H3><A name=OperationCounting>About the operation counting methodology 
</A></H3>When totaling the number of operations for algorithms here, any C 
operator is counted as one operation. Intermediate assignments, which need not 
be written to RAM, are not counted. Of course, this operation counting approach 
only serves as an approximation of the actual number of machine instructions and 
CPU time. All operations are assumed to take the same amount of time, which is 
not true in reality, but CPUs have been heading increasingly in this direction 
over time. There are many nuances that determine how fast a system will run a 
given sample of code, such as cache sizes, memory bandwidths, instruction sets, 
etc. In the end, benchmarking is the best way to determine whether one method is 
really faster than another, so consider the techniques below as possibilities to 
test on your target architecture. 
<P>
<HR>

<H3><A name=CopyIntegerSign>Compute the sign of an integer </A></H3><PRE>int v;      // we want to find the sign of v
int sign;   // the result goes here 

// CHAR_BIT is the number of bits per byte (normally 8).
sign = -(v &lt; 0);  // if v &lt; 0 then -1, else 0. 
// or, to avoid branching on CPUs with flag registers (IA32):
sign = -(int)((unsigned int)((int)v) &gt;&gt; (sizeof(int) * CHAR_BIT - 1));
// or, for one less instruction (but not portable):
sign = v &gt;&gt; (sizeof(int) * CHAR_BIT - 1); 
</PRE>The last expression above evaluates to sign = v &gt;&gt; 31 for 32-bit 
integers. This is one operation faster than the obvious way, sign = -(v &lt; 0). 
This trick works because when signed integers are shifted right, the value of 
the far left bit is copied to the other bits. The far left bit is 1 when the 
value is negative and 0 otherwise; all 1 bits gives -1. Unfortunately, this 
behavior is architecture-specific. 
<P>Alternatively, if you prefer the result be either -1 or +1, then use: <PRE>sign = +1 | (v &gt;&gt; (sizeof(int) * CHAR_BIT - 1));  // if v &lt; 0 then -1, else +1
</PRE>
<P>Alternatively, if you prefer the result be either -1, 0, or +1, then use: <PRE>sign = (v != 0) | -(int)((unsigned int)((int)v) &gt;&gt; (sizeof(int) * CHAR_BIT - 1));
// Or, for more speed but less portability:
sign = (v != 0) | (v &gt;&gt; (sizeof(int) * CHAR_BIT - 1));  // -1, 0, or +1
// Or, for portability, brevity, and (perhaps) speed:
sign = (v &gt; 0) - (v &lt; 0); // -1, 0, or +1
</PRE>Caveat: On March 7, 2003, Angus Duggan pointed out that the 1989 ANSI C 
specification leaves the result of signed right-shift implementation-defined, so 
on some systems this hack might not work. For greater portability, Toby Speight 
suggested on September 28, 2005 that CHAR_BIT be used here and throughout rather 
than assuming bytes were 8 bits long. Angus recommended the more portable 
versions above, involving casting on March 4, 2006. 
<P>
<HR>

<H3><A name=IntegerAbs>Compute the integer absolute value (abs) without 
branching </A></H3><PRE>int v;      // we want to find the absolute value of v
int r;      // the result goes here 
int const mask = v &gt;&gt; sizeof(int) * CHAR_BIT - 1;

r = (v + mask) ^ mask;
</PRE>Patented variation: <PRE>r = (v ^ mask) - mask;
</PRE>Some CPUs don't have an integer absolute value instruction (or the 
compiler fails to use them). On machines where branching is expensive, the above 
expression can be faster than the obvious approach, r = (v &lt; 0) ? -v : v, 
even though the number of operations is the same. 
<P>Caveats: On March 7, 2003, Angus Duggan pointed out that the 1989 ANSI C 
specification leaves the result of signed right-shift implementation-defined, so 
on some systems this hack might not work. I've read that ANSI C does not require 
values to be represented as two's complement, so it may not work for that reason 
as well (on a diminishingly small number of old machines that still use one's 
complement). On March 14, 2004, Keith H. Duggar sent me the patented variation 
above; it is superior to the one I initially came up with, 
<CODE>r=(+1|(v&gt;&gt;(sizeof(int)*CHAR_BIT-1)))*v</CODE>, because a multiply is 
not used. Unfortunately, this method has been <A 
href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=/netahtml/search-adv.htm&amp;r=1&amp;f=G&amp;l=50&amp;d=ptxt&amp;S1=6073150&amp;OS=6073150&amp;RS=6073150">patented</A> 
in the USA on June 6, 2000 by Vladimir Yu Volkonsky and assigned to <A 
href="http://www.sun.com/">Sun Microsystems</A>. On August 13, 2006, Yuriy 
Kaminskiy told me that the patent is likely invalid because the method was 
published well before the patent was even filed, such as in <A 
href="http://www.goof.com/pcg/doc/pentopt.txt">How to Optimize for the Pentium 
Processor</A> by Agner Fog, dated November, 9, 1996. Yuriy also mentioned that 
this document was translated to Russian in 1997, which Vladimir could have read. 
Moreover, the Internet Archive also has an old <A 
href="http://web.archive.org/web/19961201174141/www.x86.org/ftp/articles/pentopt/PENTOPT.TXT">link</A> 
to it. On January 30, 2007, Peter Kankowski shared with me an <A 
href="http://smallcode.weblogs.us/2007/01/31/microsoft-probably-uses-the-abs-function-patented-by-sun/">abs 
version</A> he discovered that was inspired by Microsoft's Visual C++ compiler 
output. It is featured here as the primary solution. 
<P>
<HR>

<H3><A name=IntegerMinOrMax>Compute the minimum (min) or maximum (max) of two 
integers without branching </A></H3><PRE>int x;  // we want to find the minimum of x and y
int y;   
int r;  // the result goes here 

r = y + ((x - y) &amp; -(x &lt; y)); // min(x, y)
</PRE>On machines where branching is expensive, the above expression can be 
faster than the obvious approach, r = (x &lt; y) ? x : y, even though it 
involves two more instructions. It works because if x&nbsp;&lt;&nbsp;y, then 
-(x&nbsp;&lt;&nbsp;y) will be all ones, so r&nbsp;= y + (x - y) &amp; ~0 = y + x 
- y = x. Otherwise, if x&nbsp;&gt;=&nbsp;y, then -(x&nbsp;&lt;&nbsp;y) will be 
all zeros, so r&nbsp;= y + (x - y) &amp; 0 = y. On some machines, evaluating (x 
&lt; y) as 0 or 1 requires a branch instruction, so there may be no advantage. 
<P>To find the maximum, use: <PRE>r = x - ((x - y) &amp; -(x &lt; y)); // max(x, y)
</PRE>
<H4>Quick and dirty versions:</H4>If you know that INT_MIN &lt;= x - y &lt;= 
INT_MAX, then you can use the following, which are faster because (x - y) only 
needs to be evaluated once. (Note that the 1989 ANSI C specification doesn't 
specify the result of signed right-shift, so these aren't portable.) <PRE>r = y + ((x - y) &amp; ((x - y) &gt;&gt; (sizeof(int) * CHAR_BIT - 1))); // min(x, y)
r = x - ((x - y) &amp; ((x - y) &gt;&gt; (sizeof(int) * CHAR_BIT - 1))); // max(x, y)
</PRE>On March 7, 2003, Angus Duggan pointed out the right-shift portability 
issue. On May 3, 2005, Randal E. Bryant alerted me to the need for the 
precondition, INT_MIN &lt;= x&nbsp;-&nbsp;y &lt;= INT_MAX, and suggested the 
non-quick and dirty version as a fix. Both of these issues concern only the 
quick and dirty version. Nigel Horspoon pointed out on July 6, 2005 that gcc 
produced the same code on a Pentium as the obvious solution because of how it 
evaluates (x &lt; y). 
<P>
<HR>

<H3><A name=DetermineIfPowerOf2>Determining if an integer is a power of 2 
</A></H3><PRE>unsigned int v; // we want to see if v is a power of 2
bool f;         // the result goes here 

f = (v &amp; (v - 1)) == 0;
</PRE>Note that 0 is incorrectly considered a power of 2 here. To remedy this, 
use: <PRE>f = !(v &amp; (v - 1)) &amp;&amp; v;
</PRE>
<HR>

<H3><A name=FixedSignExtend>Sign extending from a constant bit width 
</A></H3>Sign extension is automatic for built-in types, such as chars and ints. 
But suppose you have a signed two's complement number, x, that is stored using 
only b bits. Moreover, suppose you want to convert x to an int, which has more 
than b bits. A simple copy will work if x is positive, but if negative, the sign 
must be extended. For example, if we have only 4 bits to store a number, then -3 
is represented as 1101 in binary. If we have 8 bits, then -3 is 11111101. The 
most significant bit of the 4-bit representation is replicated sinistrally to 
fill in the destination when we convert to a representation with more bits; this 
is sign extending. In C, sign extension from a constant bit width is trivial, 
since bit fields may be specified in structs or unions. For example, to convert 
from 5 bits to an full integer: <PRE>int x; // convert this from using 5 bits to a full int
int r; // resulting sign extended number goes here
struct {signed int x:5;} s;
r = s.x = x;
</PRE>The following is a C++ template function that uses the same language 
feature to convert from B bits in one operation (though the compiler is 
generating more, of course). <PRE>template &lt;typename T, unsigned B&gt;
inline T signextend(const T x)
{
  struct {T x:B;} s;
  return s.x = x;
}

int r = signextend&lt;signed int,5&gt;(x);  // sign extend 5 bit number x to r
</PRE>
<P>John Byrd caught a typo in the code (attributed to html formatting) on May 2, 
2005. On March 4, 2006, Pat Wood pointed out that the ANSI C standard requires 
that the bitfield have the keyword "signed" to be signed; otherwise, the sign is 
undefined. 
<HR>

<H3><A name=VariableSignExtend>Sign extending from a variable bit-width 
</A></H3>Sometimes we need to extend the sign of a number but we don't know a 
priori the number of bits, b, in which it is represented. (Or we could be 
programming in Java, which lacks bitfields.) <PRE>unsigned b; // number of bits representing the number in x
int x;      // sign extend this b-bit number to r
int r;      // resulting sign-extended number
int const m = 1 &lt;&lt; (b - 1); // mask can be pre-computed if b is fixed
r = (x ^ m) - m;
</PRE>The code above requires four operations, but when the bitwidth is a 
constant rather than variable, it requires only two fast operations. 
<P>Sean A. Irvine suggested that I add sign extension methods to this page on 
June 13, 2004, and he provided <CODE>m = (1 &lt;&lt; (b - 1)) - 1; r = -(x &amp; 
~m) | x;</CODE> as a starting point from which I optimized to get m = 1 &lt;&lt; 
(b - 1); r = -(x &amp; m) | x. But then on May 11, 2007, Shay Green suggested 
the version above, which requires one less operation than mine. 
<P>
<HR>

<H3><A name=VariableSignExtendRisky>Sign extending from a variable bit-width in 
3 operations </A></H3>The following may be slow on some machines, due to the 
effort required for multiplication and division. This version is 4 operations. 
If you know that your initial bit width, b, is greater than 1, you might do this 
type of sign extension in 3 operations by using r&nbsp;= (x * multipliers[b]) / 
multipliers[b], which requires only one array lookup. <PRE>unsigned b; // number of bits representing the number in x
int x;      // sign extend this b-bit number to r
int r;      // resulting sign-extended number
#define M(B) (1 &lt;&lt; ((sizeof(x) * CHAR_BIT) - B)) // CHAR_BIT=bits/byte
static int const multipliers[] = 
{
  0,     M(1),  M(2),  M(3),  M(4),  M(5),  M(6),  M(7),
  M(8),  M(9),  M(10), M(11), M(12), M(13), M(14), M(15),
  M(16), M(17), M(18), M(19), M(20), M(21), M(22), M(23),
  M(24), M(25), M(26), M(27), M(28), M(29), M(30), M(31),
  M(32)
}; // (add more if using more than 64 bits)
static int const divisors[] = 
{
  1,    ~M(1),  M(2),  M(3),  M(4),  M(5),  M(6),  M(7),
  M(8),  M(9),  M(10), M(11), M(12), M(13), M(14), M(15),
  M(16), M(17), M(18), M(19), M(20), M(21), M(22), M(23),
  M(24), M(25), M(26), M(27), M(28), M(29), M(30), M(31),
  M(32)
}; // (add more for 64 bits)
#undef M
r = (x * multipliers[b]) / divisors[b];
</PRE>The following variation is not portable, but on architectures that employ 
an arithmetic right-shift, maintaining the sign, it should be fast. <PRE>const int s = -b; // OR:  sizeof(x) * CHAR_BIT - b;
r = (x &lt;&lt; s) &gt;&gt; s;
</PRE>Randal E. Bryant pointed out a bug on May 3, 2005 in an earlier version 
(that used multipliers[] for divisors[]), where it failed on the case of x=1 and 
b=1. 
<P>
<HR>

<H3><A name=ConditionalSetOrClearBitsWithoutBranching>Conditionally set or clear 
bits without branching </A></H3><PRE>bool f;         // conditional flag
unsigned int m; // the bit mask
unsigned int w; // the word to modify:  if (f) w |= m; else w &amp;= ~m; 

w ^= (-f ^ w) &amp; m;

// OR, for superscalar CPUs:
w = (w &amp; ~m) | (-f &amp; m);
</PRE>On some architectures, the lack of branching can more than make up for 
what appears to be twice as many operations. For instance, informal speed tests 
on an AMD Athlon™ XP 2100+ indicated it was 5-10% faster. An Intel Core 2 Duo 
ran the superscalar version about 16% faster than the first. Glenn Slayden 
informed me of the first expression on December 11, 2003. Marco Yu shared the 
superscalar version with me on April 3, 2007 and alerted me to a typo 2 days 
later. 
<P>
<HR>

<H3><A name=MaskedMerge>Merge bits from two values according to a mask </A></H3><PRE>unsigned int a;    // value to merge in non-masked bits
unsigned int b;    // value to merge in masked bits
unsigned int mask; // 1 where bits from b should be selected; 0 where from a.
unsigned int r;    // result of (a &amp; ~mask) | (b &amp; mask) goes here

r = a ^ ((a ^ b) &amp; mask); 
</PRE>This shaves one operation from the obvious way of combining two sets of 
bits according to a bit mask. If the mask is a constant, then there may be no 
advantage. 
<P>Ron Jeffery sent this to me on February 9, 2006. 
<P>
<HR>

<H3><A name=CountBitsSetNaive>Counting bits set (naive way) </A></H3><PRE>unsigned int v; // count the number of bits set in v
unsigned int c; // c accumulates the total bits set in v

for (c = 0; v; v &gt;&gt;= 1)
{
  c += v &amp; 1;
}
</PRE>The naive approach requires one iteration per bit, until no more bits are 
set. So on a 32-bit word with only the high set, it will go through 32 
iterations. 
<P>
<HR>

<H3><A name=CountBitsSetTable>Counting bits set by lookup table </A></H3><PRE>static const unsigned char BitsSetTable256[] = 
{
  0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 
  1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 
  1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 
  2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 
  1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 
  2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 
  2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 
  3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 
  1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 
  2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 
  2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 
  3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 
  2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 
  3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 
  3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 
  4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8
};

unsigned int v; // count the number of bits set in 32-bit value v
unsigned int c; // c is the total bits set in v

// Option 1:
c = BitsSetTable256[v &amp; 0xff] + 
    BitsSetTable256[(v &gt;&gt; 8) &amp; 0xff] + 
    BitsSetTable256[(v &gt;&gt; 16) &amp; 0xff] + 
    BitsSetTable256[v &gt;&gt; 24]; 

// Option 2:
unsigned char * p = (unsigned char *) &amp;v;
c = BitsSetTable256[p[0]] + 
    BitsSetTable256[p[1]] + 
    BitsSetTable256[p[2]] +	
    BitsSetTable256[p[3]];


// To initially generate the table algorithmically:
BitsSetTable256[0] = 0;
for (int i = 0; i &lt; 256; i++)
{
  BitsSetTable256[i] = (i &amp; 1) + BitsSetTable256[i / 2];
}
</PRE>
<P>
<HR>

<H3><A name=CountBitsSetKernighan>Counting bits set, Brian Kernighan's way 
</A></H3><PRE>unsigned int v; // count the number of bits set in v
unsigned int c; // c accumulates the total bits set in v
for (c = 0; v; c++)
{
  v &amp;= v - 1; // clear the least significant bit set
}
</PRE>Brian Kernighan's method goes through as many iterations as there are set 
bits. So if we have a 32-bit word with only the high bit set, then it will only 
go once through the loop. 
<P>Published in 1988, the C Programming Language 2nd Ed. (by Brian W. Kernighan 
and Dennis M. Ritchie) mentions this in exercise 2-9. On April 19, 2006 Don 
Knuth pointed out to me that this method "was first published by Peter Wegner in 
CACM 3 (1960), 322. (Also discovered independently by Derrick Lehmer and 
published in 1964 in a book edited by Beckenbach.)" 
<P>
<HR>

<H3><A name=CountBitsSet64>Counting bits set in 14, 24, or 32-bit words using 
64-bit instructions </A></H3><PRE>unsigned int v; // count the number of bits set in v
unsigned int c; // c accumulates the total bits set in v

// option 1, for at most 14-bit values in v:
c = (v * 0x200040008001ULL &amp; 0x111111111111111ULL) % 0xf;

// option 2, for at most 24-bit values in v:
c =  ((v &amp; 0xfff) * 0x1001001001001ULL &amp; 0x84210842108421ULL) % 0x1f;
c += (((v &amp; 0xfff000) &gt;&gt; 12) * 0x1001001001001ULL &amp; 0x84210842108421ULL) 
     % 0x1f;

// option 3, for at most 32-bit values in v:
c =  ((v &amp; 0xfff) * 0x1001001001001ULL &amp; 0x84210842108421ULL) % 0x1f;
c += (((v &amp; 0xfff000) &gt;&gt; 12) * 0x1001001001001ULL &amp; 0x84210842108421ULL) % 
     0x1f;
c += ((v &gt;&gt; 24) * 0x1001001001001ULL &amp; 0x84210842108421ULL) % 0x1f;
</PRE>This method requires a 64-bit CPU with fast modulus division to be 
efficient. The first option takes only 3 operations; the second option takes 10; 
and the third option takes 15. 
<P>Rich Schroeppel originally created a 9-bit version, similiar to option 1; see 
the Programming Hacks section of <A 
href="http://www.inwap.com/pdp10/hbaker/hakmem/hakmem.html">Beeler, M., Gosper, 
R. W., and Schroeppel, R. HAKMEM. MIT AI Memo 239, Feb. 29, 1972.</A> His method 
was the inspiration for the variants above, devised by Sean Anderson. Randal E. 
Bryant offered a couple bug fixes on May 3, 2005. Bruce Dawson tweaked what had 
been a 12-bit version and made it suitable for 14 bits using the same number of 
operations on Feburary 1, 2007. 
<P><!-- After reading this and pondering it, Roshan James wrote a web page
<a href=
"http://pensieve.thinkingms.com/sparksite/articles/tech/bitcounter64/bitcounter_64bit.htm">explaining how it works</a> and 
<a href="http://pensieve.thinkingms.com/sparksite/articles/tech/bitcounter64/bitcounter_64bit_2.htm">extending it to
16 bits</a> on March 8, 2004. -->
<P>
<HR>

<H3><A name=CountBitsSetParallel>Counting bits set, in parallel </A></H3><PRE>unsigned int v; // count bits set in this (32-bit value)
unsigned int c; // store the total here
static const int S[] = {1, 2, 4, 8, 16}; // Magic Binary Numbers
static const int B[] = {0x55555555, 0x33333333, 0x0F0F0F0F, 0x00FF00FF, 0x0000FFFF};

c = v - ((v &gt;&gt; 1) &amp; B[0]);
c = ((c &gt;&gt; S[1]) &amp; B[1]) + (c &amp; B[1]);
c = ((c &gt;&gt; S[2]) + c) &amp; B[2];
c = ((c &gt;&gt; S[3]) + c) &amp; B[3];
c = ((c &gt;&gt; S[4]) + c) &amp; B[4];
</PRE>The B array, expressed as binary, is: <PRE>B[0] = 0x55555555 = 01010101 01010101 01010101 01010101
B[1] = 0x33333333 = 00110011 00110011 00110011 00110011
B[2] = 0x0F0F0F0F = 00001111 00001111 00001111 00001111
B[3] = 0x00FF00FF = 00000000 11111111 00000000 11111111
B[4] = 0x0000FFFF = 00000000 00000000 11111111 11111111
</PRE>We can adjust the method for larger integer sizes by continuing with the 
patterns for the <EM>Binary Magic Numbers,</EM> B and S. If there are k bits, 
then we need the arrays S and B to be ceil(lg(k)) elements long, and we must 
compute the same number of expressions for c as S or B are long. For a 32-bit v, 
16 operations are used. 
<P>The best method for counting bits in a 32-bit integer v is the following: <PRE>v = v - ((v &gt;&gt; 1) &amp; 0x55555555);                    // reuse input as temporary
v = (v &amp; 0x33333333) + ((v &gt;&gt; 2) &amp; 0x33333333);     // temp
c = ((v + (v &gt;&gt; 4) &amp; 0xF0F0F0F) * 0x1010101) &gt;&gt; 24; // count
</PRE>
<P>The best bit counting method takes only 12 operations, which is the same as 
the lookup-table method, but avoids the memory and potential cache misses of a 
table. It is a hybrid between the purely parallel method above and the earlier 
methods using multiplies (in the section on counting bits with 64-bit 
instructions), though it doesn't use 64-bit instructions. The counts of bits set 
in the bytes is done in parallel, and the sum total of the bits set in the bytes 
is computed by multiplying by 0x1010101 and shifting right 24 bits. 
<P>A generalization of the best bit counting method to integers of bit widths 
upto 128 (parameterized by type T) is this: <PRE>v = v - ((v &gt;&gt; 1) &amp; (T)~(T)0/3);                           // temp
v = (v &amp; (T)~(T)0/15*3) + ((v &gt;&gt; 2) &amp; (T)~(T)0/15*3);      // temp
v = (v + (v &gt;&gt; 4)) &amp; (T)~(T)0/255*15;                      // temp
c = (T)(v * ((T)~(T)0/255)) &gt;&gt; (sizeof(v) - 1) * CHAR_BIT; // count
</PRE>
<P>See <A 
href="http://groups.google.com/groups?q=reverse+bits&amp;num=100&amp;hl=en&amp;group=comp.graphics.algorithms&amp;imgsafe=off&amp;safe=off&amp;rnum=2&amp;ic=1&amp;selm=4fulhm%248dn%40atlas.uniserve.com">Ian 
Ashdown's nice newsgroup post</A> for more information on counting the number of 
bits set (also known as <EM>sideways addition</EM>). The best bit counting 
method was brought to my attention on October 5, 2005 by <A 
href="http://onezero.org/">Andrew Shapira</A>; he found it in pages 187-188 of 
<A 
href="http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/25112.PDF">Software 
Optimization Guide for AMD Athlon™ 64 and Opteron™ Processors</A>. Charlie 
Gordon suggested a way to shave off one operation from the purely parallel 
version on December 14, 2005, and Don Clugston trimmed three more from it on 
December 30, 2005. He also pointed out that if rotate operations were available, 
changing the shifts to rotates would eliminate the need for the AND masks 
because the overcounted sum total could instead be right-shifted 3 at the end. I 
made a typo with Don's suggestion that Eric Cole spotted on January 8, 2006. 
Eric later suggested the arbitrary bit-width generalization to the best method 
on November 17, 2006. On April 5, 2007, Al Williams observed that I had a line 
of dead code at the top of the first method. 
<P>
<HR>

<H3><A name=ParityNaive>Computing parity the naive way </A></H3><PRE>unsigned int v;       // word value to compute the parity of
bool parity = false;  // parity will be the parity of b

while (v)
{
  parity = !parity;
  v = v &amp; (v - 1);
}

</PRE>The above code uses an approach like Brian Kernigan's bit counting, above. 
The time it takes is proportional to the number of bits set. 
<P>
<HR>

<H3><A name=ParityWith64Bits>Compute parity of a byte using 64-bit multiply and 
modulus division </A></H3><PRE>unsigned char b;  // byte value to compute the parity of
bool parity = 
  (((b * 0x0101010101010101ULL) &amp; 0x8040201008040201ULL) % 0x1FF) &amp; 1;
</PRE>The method above takes around 4 operations, but only works on bytes. 
<P>
<HR>

<H3><A name=ParityParallel>Compute parity in parallel </A></H3><PRE>unsigned int v;  // word value to compute the parity of
v ^= v &gt;&gt; 16;
v ^= v &gt;&gt; 8;
v ^= v &gt;&gt; 4;
v &amp;= 0xf;
return (0x6996 &gt;&gt; v) &amp; 1;
</PRE>The method above takes around 9 operations, and works for 32-bit words. It 
may be optimized to work just on bytes in 5 operations by removing the two lines 
immediately following "unsigned int v;". The method first shifts and XORs the 
eight nibbles of the 32-bit value together, leaving the result in the lowest 
nibble of v. Next, the binary number 0110 1001 1001 0110 (0x6996 in hex) is 
shifted to the right by the value represented in the lowest nibble of v. This 
number is like a miniature 16-bit parity-table indexed by the low four bits in 
v. The result has the parity of v in bit 1, which is masked and returned. 
<P>Thanks to Mathew Hendry for pointing out the shift-lookup idea at the end on 
Dec. 15, 2002. That optimization shaves two operations off using only shifting 
and XORing to find the parity. 
<P>
<HR>

<H3><A name=ParityLookupTable>Compute parity by lookup table </A></H3><PRE>static const bool ParityTable[] = 
{
  0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 
  1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 
  1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 
  0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 
  1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 
  0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 
  0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 
  1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 
  1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 
  0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 
  0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 
  1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 
  0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 
  1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 
  1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 
  0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0
};

unsigned char b;  // byte value to compute the parity of
bool parity = ParityTable[b];

// OR, for 32-bit words:
v ^= v &gt;&gt; 16;
v ^= v &gt;&gt; 8;
bool parity = ParityTable[v &amp; 0xff];

// Variation:
unsigned char * p = (unsigned char *) &amp;v;
parity = ParityTable[p[0] ^ p[1] ^ p[2] ^ p[3]];

</PRE>Randal E. Bryant encouraged the addition of the (admittedly) obvious last 
variation with variable p on May 3, 2005. Bruce Rawles found a typo in an 
instance of the table variable's name on September 27, 2005, and he received a 
$10 bug bounty. On October 9, 2006, Fabrice Bellard suggested the 32-bit 
variations above, which require only one table lookup; the previous version had 
four lookups (one per byte) and were slower. 
<P>
<HR>

<H3><A name=SwappingValuesSubAdd>Swapping values with subtraction and addition 
</A></H3><PRE>#define SWAP(a, b) ((&amp;(a) == &amp;(b)) || \
                    (((a) -= (b)), ((b) += (a)), ((a) = (b) - (a))))
</PRE>This swaps the values of a and b <EM>without using a temporary 
variable.</EM> The initial check for a and b being the same location in memory 
may be omitted when you know this can't happen. (The compiler may omit it anyway 
as an optimization.) The XOR method that follows may be slightly faster on some 
machines. 
<P>Sanjeev Sivasankaran suggested I add this on June 12, 2007. 
<HR>

<H3><A name=SwappingValuesXOR>Swapping values with XOR </A></H3><PRE>#define SWAP(a, b) (((a) ^= (b)), ((b) ^= (a)), ((a) ^= (b)))
</PRE>This is an old trick to exchange the values of the variables a and b 
<EM>without using extra space for a temporary variable</EM>. 
<P>On January 20, 2005, Iain A. Fleming pointed out that the macro above doesn't 
work when you swap with the same memory location, such as SWAP(a[i], a[j]) with 
i == j. So if that may occur, consider defining the macro as (((a) == (b)) || 
(((a) ^= (b)), ((b) ^= (a)), ((a) ^= (b)))). 
<P>
<HR>

<H3><A name=SwappingBitsXOR>Swapping individual bits with XOR </A></H3><PRE>unsigned int i, j; // positions of bit sequences to swap
unsigned int n;    // number of consecutive bits in each sequence
unsigned int b;    // bits to swap reside in b
unsigned int r;    // bit-swapped result goes here

int x = ((b &gt;&gt; i) ^ (b &gt;&gt; j)) &amp; ((1 &lt;&lt; n) - 1); // XOR temporary
r = b ^ ((x &lt;&lt; i) | (x &lt;&lt; j));
</PRE>As an example of swapping ranges of bits suppose we have have b = 
<B>001</B>0<B>111</B>1 (expressed in binary) and we want to swap the n = 3 
consecutive bits starting at i = 1 (the second bit from the right) with the 3 
consecutive bits starting at j = 5; the result would be r = 
<B>111</B>0<B>001</B>1 (binary). 
<P>This method of swapping is similar to the general purpose XOR swap trick, but 
intended for operating on individual bits.&nbsp; The variable x stores the 
result of XORing the pairs of bit values we want to swap, and then the bits are 
set to the result of themselves XORed with x.&nbsp; Of course, the result is 
undefined if the sequences overlap. 
<P>
<HR>

<H3><A name=BitReverseObvious>Reverse bits the obvious way </A></H3><PRE>unsigned int v;     // input bits to be reversed
unsigned int r = v; // r will be reversed bits of v; first get LSB of v
int s = sizeof(v) * CHAR_BIT - 1; // extra shift needed at end

for (v &gt;&gt;= 1; v; v &gt;&gt;= 1)
{   
  r &lt;&lt;= 1;
  r |= v &amp; 1;
  s--;
}
r &lt;&lt;= s; // shift when v's highest bits are zero
</PRE>
<P>On October 15, 2004, Michael Hoisie pointed out a bug in the original 
version. Randal E. Bryant suggested removing an extra operation on May 3, 2005. 
Behdad Esfabod suggested a slight change that eliminated one iteration of the 
loop on May 18, 2005. Then, on February 6, 2007, Liyong Zhou suggested a better 
version that loops while v is not 0, so rather than iterating over all bits it 
stops early. 
<P>
<HR>

<H3><A name=BitReverseTable>Reverse bits in word by lookup table </A></H3><PRE>static const unsigned char BitReverseTable256[] = 
{
  0x00, 0x80, 0x40, 0xC0, 0x20, 0xA0, 0x60, 0xE0, 0x10, 0x90, 0x50, 0xD0, 0x30, 0xB0, 0x70, 0xF0, 
  0x08, 0x88, 0x48, 0xC8, 0x28, 0xA8, 0x68, 0xE8, 0x18, 0x98, 0x58, 0xD8, 0x38, 0xB8, 0x78, 0xF8, 
  0x04, 0x84, 0x44, 0xC4, 0x24, 0xA4, 0x64, 0xE4, 0x14, 0x94, 0x54, 0xD4, 0x34, 0xB4, 0x74, 0xF4, 
  0x0C, 0x8C, 0x4C, 0xCC, 0x2C, 0xAC, 0x6C, 0xEC, 0x1C, 0x9C, 0x5C, 0xDC, 0x3C, 0xBC, 0x7C, 0xFC, 
  0x02, 0x82, 0x42, 0xC2, 0x22, 0xA2, 0x62, 0xE2, 0x12, 0x92, 0x52, 0xD2, 0x32, 0xB2, 0x72, 0xF2, 
  0x0A, 0x8A, 0x4A, 0xCA, 0x2A, 0xAA, 0x6A, 0xEA, 0x1A, 0x9A, 0x5A, 0xDA, 0x3A, 0xBA, 0x7A, 0xFA,
  0x06, 0x86, 0x46, 0xC6, 0x26, 0xA6, 0x66, 0xE6, 0x16, 0x96, 0x56, 0xD6, 0x36, 0xB6, 0x76, 0xF6, 
  0x0E, 0x8E, 0x4E, 0xCE, 0x2E, 0xAE, 0x6E, 0xEE, 0x1E, 0x9E, 0x5E, 0xDE, 0x3E, 0xBE, 0x7E, 0xFE,
  0x01, 0x81, 0x41, 0xC1, 0x21, 0xA1, 0x61, 0xE1, 0x11, 0x91, 0x51, 0xD1, 0x31, 0xB1, 0x71, 0xF1,
  0x09, 0x89, 0x49, 0xC9, 0x29, 0xA9, 0x69, 0xE9, 0x19, 0x99, 0x59, 0xD9, 0x39, 0xB9, 0x79, 0xF9, 
  0x05, 0x85, 0x45, 0xC5, 0x25, 0xA5, 0x65, 0xE5, 0x15, 0x95, 0x55, 0xD5, 0x35, 0xB5, 0x75, 0xF5,
  0x0D, 0x8D, 0x4D, 0xCD, 0x2D, 0xAD, 0x6D, 0xED, 0x1D, 0x9D, 0x5D, 0xDD, 0x3D, 0xBD, 0x7D, 0xFD,
  0x03, 0x83, 0x43, 0xC3, 0x23, 0xA3, 0x63, 0xE3, 0x13, 0x93, 0x53, 0xD3, 0x33, 0xB3, 0x73, 0xF3, 
  0x0B, 0x8B, 0x4B, 0xCB, 0x2B, 0xAB, 0x6B, 0xEB, 0x1B, 0x9B, 0x5B, 0xDB, 0x3B, 0xBB, 0x7B, 0xFB,
  0x07, 0x87, 0x47, 0xC7, 0x27, 0xA7, 0x67, 0xE7, 0x17, 0x97, 0x57, 0xD7, 0x37, 0xB7, 0x77, 0xF7, 
  0x0F, 0x8F, 0x4F, 0xCF, 0x2F, 0xAF, 0x6F, 0xEF, 0x1F, 0x9F, 0x5F, 0xDF, 0x3F, 0xBF, 0x7F, 0xFF
};

unsigned int v; // reverse 32-bit value, 8 bits at time
unsigned int c; // c will get v reversed

// Option 1:
c = (BitReverseTable256[v &amp; 0xff] &lt;&lt; 24) | 
    (BitReverseTable256[(v &gt;&gt; 8) &amp; 0xff] &lt;&lt; 16) | 
    (BitReverseTable256[(v &gt;&gt; 16) &amp; 0xff] &lt;&lt; 8) |
    (BitReverseTable256[(v &gt;&gt; 24) &amp; 0xff]);

// Option 2:
unsigned char * p = (unsigned char *) &amp;v;
unsigned char * q = (unsigned char *) &amp;c;
q[3] = BitReverseTable256[p[0]]; 
q[2] = BitReverseTable256[p[1]]; 
q[1] = BitReverseTable256[p[2]]; 
q[0] = BitReverseTable256[p[3]];
</PRE>The first method takes about 17 operations, and the second takes about 12, 
assuming your CPU can load and store bytes easily. 
<P>
<HR>

<H3><A name=ReverseByteWith64BitsDiv>Reverse the bits in a byte with 3 
operations (64-bit multiply and modulus division): </A></H3><PRE>unsigned char b; // reverse this (8-bit) byte
 
b = (b * 0x0202020202ULL &amp; 0x010884422010ULL) % 1023;
</PRE>The multiply operation creates five separate copies of the 8-bit byte 
pattern to fan-out into a 64-bit value. The AND operation selects the bits that 
are in the correct (reversed) positions, relative to each 10-bit groups of bits. 
The multiply and the AND operations copy the bits from the original byte so they 
each appear in only one of the 10-bit sets. The reversed positions of the bits 
from the original byte coincide with their relative positions within any 10-bit 
set. The last step, which involves modulus division by 2^10 - 1, has the effect 
of merging together each set of 10 bits (from positions 0-9, 10-19, 20-29, ...) 
in the 64-bit value. They do not overlap, so the addition steps underlying the 
modulus division behave like or operations. 
<P>This method was attributed to Rich Schroeppel in the Programming Hacks 
section of <A 
href="http://www.inwap.com/pdp10/hbaker/hakmem/hakmem.html">Beeler, M., Gosper, 
R. W., and Schroeppel, R. HAKMEM. MIT AI Memo 239, Feb. 29, 1972.</A> 
<P>
<HR>

<H3><A name=ReverseByteWith64Bits>Reverse the bits in a byte with 4 operations 
(64-bit multiply, no division): </A></H3><PRE>unsigned char b; // reverse this byte
 
b = ((b * 0x80200802ULL) &amp; 0x0884422110ULL) * 0x0101010101ULL &gt;&gt; 32;
</PRE>The following shows the flow of the bit values with the boolean variables 
<CODE>a, b, c, d, e, f, g,</CODE> and <CODE>h</CODE>, which comprise an 8-bit 
byte. Notice how the first multiply fans out the bit pattern to multiple copies, 
while the last multiply combines them in the fifth byte from the right. <FONT 
size=-1><PRE>                                                                                        abcd efgh (-&gt; hgfe dcba)
*                                                      1000 0000  0010 0000  0000 1000  0000 0010 (0x80200802)
-------------------------------------------------------------------------------------------------
                                            0abc defg  h00a bcde  fgh0 0abc  defg h00a  bcde fgh0
&amp;                                           0000 1000  1000 0100  0100 0010  0010 0001  0001 0000 (0x0884422110)
-------------------------------------------------------------------------------------------------
                                            0000 d000  h000 0c00  0g00 00b0  00f0 000a  000e 0000
*                                           0000 0001  0000 0001  0000 0001  0000 0001  0000 0001 (0x0101010101)
-------------------------------------------------------------------------------------------------
                                            0000 d000  h000 0c00  0g00 00b0  00f0 000a  000e 0000
                                 0000 d000  h000 0c00  0g00 00b0  00f0 000a  000e 0000
                      0000 d000  h000 0c00  0g00 00b0  00f0 000a  000e 0000
           0000 d000  h000 0c00  0g00 00b0  00f0 000a  000e 0000
0000 d000  h000 0c00  0g00 00b0  00f0 000a  000e 0000
-------------------------------------------------------------------------------------------------
0000 d000  h000 dc00  hg00 dcb0  hgf0 dcba  hgfe dcba  hgfe 0cba  0gfe 00ba  00fe 000a  000e 0000
&gt;&gt; 32
-------------------------------------------------------------------------------------------------
                                            0000 d000  h000 dc00  hg00 dcb0  hgf0 dcba  hgfe dcba  
&amp;                                                                                       1111 1111
-------------------------------------------------------------------------------------------------
                                                                                        hgfe dcba
</PRE></FONT>Note that the last two steps can be combined on some processors 
because the registers can be accessed as bytes; just multiply so that a register 
stores the upper 32 bits of the result and the take the low byte. Thus, it may 
take only 6 operations. 
<P>Devised by Sean Anderson, July 13, 2001. 
<P>
<HR>

<H3><A name=ReverseByteWith32Bits>Reverse the bits in a byte with 7 operations 
(no 64-bit): </A></H3><PRE>b = ((b * 0x0802LU &amp; 0x22110LU) | (b * 0x8020LU &amp; 0x88440LU)) * 0x10101LU &gt;&gt; 16
</PRE>Devised by Sean Anderson, July 13, 2001. Typo spotted and correction 
supplied by Mike Keith, January 3, 2002. 
<P>
<HR>

<H3><A name=ReverseParallel>Reverse an N-bit quantity in parallel in 5 * lg(N) 
operations: </A></H3><PRE>unsigned int v; // 32 bit word to reverse bit order

// swap odd and even bits
v = ((v &gt;&gt; 1) &amp; 0x55555555) | ((v &amp; 0x55555555) &lt;&lt; 1);
// swap consecutive pairs
v = ((v &gt;&gt; 2) &amp; 0x33333333) | ((v &amp; 0x33333333) &lt;&lt; 2);
// swap nibbles ... 
v = ((v &gt;&gt; 4) &amp; 0x0F0F0F0F) | ((v &amp; 0x0F0F0F0F) &lt;&lt; 4);
// swap bytes
v = ((v &gt;&gt; 8) &amp; 0x00FF00FF) | ((v &amp; 0x00FF00FF) &lt;&lt; 8);
// swap 2-byte long pairs
v = ( v &gt;&gt; 16             ) | ( v               &lt;&lt; 16);
</PRE>The following variation is also O(lg(N)), however it requires more 
operations to reverse v. Its virtue is in taking less slightly memory by 
computing the constants on the fly. <PRE>unsigned int s = sizeof(v) * CHAR_BIT; // bit size; must be power of 2 
unsigned int mask = ~0;         
while ((s &gt;&gt;= 1) &gt; 0) 
{
  mask ^= (mask &lt;&lt; s);
  v = ((v &gt;&gt; s) &amp; mask) | ((v &lt;&lt; s) &amp; ~mask);
}
</PRE>These methods above are best suited to situations where N is large. 
<P>See Dr. Dobb's Journal 1983, Edwin Freed's article on Binary Magic Numbers 
for more information. The second variation was suggested by Ken Raeburn on 
September 13, 2005. Veldmeijer mentioned that the first version could do without 
ANDS in the last line on March 19, 2006. 
<P>
<HR>

<H3><A name=ModulusDivisionEasy>Compute modulus division by 1 &lt;&lt; s without 
a division operator </A></H3><PRE>const unsigned int n;          // numerator
const unsigned int s;
const unsigned int d = 1 &lt;&lt; s; // So d will be one of: 1, 2, 4, 8, 16, 32, ...
unsigned int m;                // m will be n % d
m = n &amp; (d - 1); 
</PRE>Most programmers learn this trick early, but it was included for the sake 
of completeness. 
<P>
<HR>

<P>
<H3><A name=ModulusDivision>Compute modulus division by (1 &lt;&lt; s) - 1 
without a division operator </A></H3><PRE>unsigned int n;                      // numerator
const unsigned int s;                // s &gt; 0
const unsigned int d = (1 &lt;&lt; s) - 1; // so d is either 1, 3, 7, 15, 31, ...).
unsigned int m;                      // n % d goes here.

for (m = n; n &gt; d; n = m)
{
  for (m = 0; n; n &gt;&gt;= s)
  {
    m += n &amp; d;
  }
}
// Now m is a value from 0 to d, but since with modulus division
// we want m to be 0 when it is d.
m = m == d ? 0 : m;
</PRE>This method of modulus division by an integer that is one less than a 
power of 2 takes at most 5 + (4 + 5 * ceil(N / s)) * ceil(lg(N / s)) operations, 
where N is the number of bits in the numerator. In other words, it takes at most 
O(N * lg(N)) time. 
<P>Devised by Sean Anderson, August 15, 2001. Before Sean A. Irvine corrected me 
on June 17, 2004, I mistakenly commented that we could alternatively assign 
<CODE>m = ((m + 1) &amp; d) - 1;</CODE> at the end. Michael Miller spotted a 
typo in the code April 25, 2005. 
<P>
<HR>

<H3><A name=ModulusDivisionParallel>Compute modulus division by (1 &lt;&lt; s) - 
1 in parallel without a division operator </A></H3><PRE>
// The following is for a word size of 32 bits!

static const unsigned int M[] = 
{
  0x00000000, 0x55555555, 0x33333333, 0xc71c71c7,  
  0x0f0f0f0f, 0xc1f07c1f, 0x3f03f03f, 0xf01fc07f, 
  0x00ff00ff, 0x07fc01ff, 0x3ff003ff, 0xffc007ff,
  0xff000fff, 0xfc001fff, 0xf0003fff, 0xc0007fff,
  0x0000ffff, 0x0001ffff, 0x0003ffff, 0x0007ffff, 
  0x000fffff, 0x001fffff, 0x003fffff, 0x007fffff,
  0x00ffffff, 0x01ffffff, 0x03ffffff, 0x07ffffff,
  0x0fffffff, 0x1fffffff, 0x3fffffff, 0x7fffffff
};

static const unsigned int Q[][6] = 
{
  { 0,  0,  0,  0,  0,  0}, {16,  8,  4,  2,  1,  1}, {16,  8,  4,  2,  2,  2},
  {15,  6,  3,  3,  3,  3}, {16,  8,  4,  4,  4,  4}, {15,  5,  5,  5,  5,  5},
  {12,  6,  6,  6 , 6,  6}, {14,  7,  7,  7,  7,  7}, {16,  8,  8,  8,  8,  8},
  { 9,  9,  9,  9,  9,  9}, {10, 10, 10, 10, 10, 10}, {11, 11, 11, 11, 11, 11},
  {12, 12, 12, 12, 12, 12}, {13, 13, 13, 13, 13, 13}, {14, 14, 14, 14, 14, 14},
  {15, 15, 15, 15, 15, 15}, {16, 16, 16, 16, 16, 16}, {17, 17, 17, 17, 17, 17},
  {18, 18, 18, 18, 18, 18}, {19, 19, 19, 19, 19, 19}, {20, 20, 20, 20, 20, 20},
  {21, 21, 21, 21, 21, 21}, {22, 22, 22, 22, 22, 22}, {23, 23, 23, 23, 23, 23},
  {24, 24, 24, 24, 24, 24}, {25, 25, 25, 25, 25, 25}, {26, 26, 26, 26, 26, 26},
  {27, 27, 27, 27, 27, 27}, {28, 28, 28, 28, 28, 28}, {29, 29, 29, 29, 29, 29},
  {30, 30, 30, 30, 30, 30}, {31, 31, 31, 31, 31, 31}
};

static const unsigned int R[][6] = 
{
  {0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000},
  {0x0000ffff, 0x000000ff, 0x0000000f, 0x00000003, 0x00000001, 0x00000001},
  {0x0000ffff, 0x000000ff, 0x0000000f, 0x00000003, 0x00000003, 0x00000003},
  {0x00007fff, 0x0000003f, 0x00000007, 0x00000007, 0x00000007, 0x00000007},
  {0x0000ffff, 0x000000ff, 0x0000000f, 0x0000000f, 0x0000000f, 0x0000000f},
  {0x00007fff, 0x0000001f, 0x0000001f, 0x0000001f, 0x0000001f, 0x0000001f},
  {0x00000fff, 0x0000003f, 0x0000003f, 0x0000003f, 0x0000003f, 0x0000003f},
  {0x00003fff, 0x0000007f, 0x0000007f, 0x0000007f, 0x0000007f, 0x0000007f},
  {0x0000ffff, 0x000000ff, 0x000000ff, 0x000000ff, 0x000000ff, 0x000000ff},
  {0x000001ff, 0x000001ff, 0x000001ff, 0x000001ff, 0x000001ff, 0x000001ff}, 
  {0x000003ff, 0x000003ff, 0x000003ff, 0x000003ff, 0x000003ff, 0x000003ff}, 
  {0x000007ff, 0x000007ff, 0x000007ff, 0x000007ff, 0x000007ff, 0x000007ff}, 
  {0x00000fff, 0x00000fff, 0x00000fff, 0x00000fff, 0x00000fff, 0x00000fff}, 
  {0x00001fff, 0x00001fff, 0x00001fff, 0x00001fff, 0x00001fff, 0x00001fff}, 
  {0x00003fff, 0x00003fff, 0x00003fff, 0x00003fff, 0x00003fff, 0x00003fff}, 
  {0x00007fff, 0x00007fff, 0x00007fff, 0x00007fff, 0x00007fff, 0x00007fff}, 
  {0x0000ffff, 0x0000ffff, 0x0000ffff, 0x0000ffff, 0x0000ffff, 0x0000ffff}, 
  {0x0001ffff, 0x0001ffff, 0x0001ffff, 0x0001ffff, 0x0001ffff, 0x0001ffff}, 
  {0x0003ffff, 0x0003ffff, 0x0003ffff, 0x0003ffff, 0x0003ffff, 0x0003ffff}, 
  {0x0007ffff, 0x0007ffff, 0x0007ffff, 0x0007ffff, 0x0007ffff, 0x0007ffff},
  {0x000fffff, 0x000fffff, 0x000fffff, 0x000fffff, 0x000fffff, 0x000fffff}, 
  {0x001fffff, 0x001fffff, 0x001fffff, 0x001fffff, 0x001fffff, 0x001fffff}, 
  {0x003fffff, 0x003fffff, 0x003fffff, 0x003fffff, 0x003fffff, 0x003fffff}, 
  {0x007fffff, 0x007fffff, 0x007fffff, 0x007fffff, 0x007fffff, 0x007fffff}, 
  {0x00ffffff, 0x00ffffff, 0x00ffffff, 0x00ffffff, 0x00ffffff, 0x00ffffff},
  {0x01ffffff, 0x01ffffff, 0x01ffffff, 0x01ffffff, 0x01ffffff, 0x01ffffff}, 
  {0x03ffffff, 0x03ffffff, 0x03ffffff, 0x03ffffff, 0x03ffffff, 0x03ffffff}, 
  {0x07ffffff, 0x07ffffff, 0x07ffffff, 0x07ffffff, 0x07ffffff, 0x07ffffff},
  {0x0fffffff, 0x0fffffff, 0x0fffffff, 0x0fffffff, 0x0fffffff, 0x0fffffff},
  {0x1fffffff, 0x1fffffff, 0x1fffffff, 0x1fffffff, 0x1fffffff, 0x1fffffff}, 
  {0x3fffffff, 0x3fffffff, 0x3fffffff, 0x3fffffff, 0x3fffffff, 0x3fffffff}, 
  {0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff}
};

unsigned int n;       // numerator
const unsigned int s; // s &gt; 0
const unsigned int d = (1 &lt;&lt; s) - 1; // so d is either 1, 3, 7, 15, 31, ...).
unsigned int m;       // n % d goes here.

m = (n &amp; M[s]) + ((n &amp; ~M[s]) &gt;&gt; s);

for (const unsigned int * q = &amp;Q[s][0], * r = &amp;R[s][0]; m &gt; d; q++, r++)
{
  m = (m &gt;&gt; *q) + (m &amp; *r);
}
m = m == d ? 0 : m; // OR, less portably: m = m &amp; -((signed)(m - d) &gt;&gt; s);
</PRE>This method of finding modulus division by an integer that is one less 
than a power of 2 takes at most O(lg(N)) time, where N is the number of bits in 
the numerator (32 bits, for the code above). The number of operations is at most 
12 + 9 * ceil(lg(N)). The tables may be removed if you know the denominator at 
compile time; just extract the few relevent entries and unroll the loop. It may 
be easily extended to more bits. 
<P>It finds the result by summing the values in base (1 &lt;&lt; s) in parallel. 
First every other base (1 &lt;&lt; s) value is added to the previous one. 
Imagine that the result is written on a piece of paper. Cut the paper in half, 
so that half the values are on each cut piece. Align the values and sum them 
onto a new piece of paper. Repeat by cutting this paper in half (which will be a 
quarter of the size of the previous one) and summing, until you cannot cut 
further. After performing lg(N/s/2) cuts, we cut no more; just continue to add 
the values and put the result onto a new piece of paper as before, while there 
are at least two s-bit values. 
<P>Devised by Sean Anderson, August 20, 2001. A typo was spotted by Randy E. 
Bryant on May 3, 2005 (after pasting the code, I had later added "unsinged" to a 
variable declaration). As in the previous hack, I mistakenly commented that we 
could alternatively assign <CODE>m = ((m + 1) &amp; d) - 1;</CODE> at the end, 
and Don Knuth corrected me on April 19, 2006 and suggested <CODE>m = m &amp; 
-((signed)(m - d) &gt;&gt; s)</CODE>. 
<P>
<HR>

<H3><A name=IntegerLogObvious>Find the log base 2 of an integer with the MSB N 
set in O(N) operations (the obvious way) </A></H3><PRE>unsigned int v; // 32-bit word to find the log base 2 of
unsigned r = 0; // r will be lg(v)

while (v &gt;&gt;= 1) // unroll for more speed...
{
  r++;
}
</PRE>The log base 2 of an integer is the same as the position of the highest 
bit set (or most significant bit set, MSB). The following log base 2 methods are 
faster than this one. 
<P>
<HR>

<H3><A name=IntegerLogIEEE64Float>Find the integer log base 2 of an integer with 
an 64-bit IEEE float </A></H3><PRE>int v; // 32-bit integer to find the log base 2 of
int r; // result of log_2(v) goes here
union { unsigned int u[2]; double d; } t; // temp

t.u[BYTE_ORDER==LITTLE_ENDIAN] = 0x43300000;
t.u[BYTE_ORDER!=LITTLE_ENDIAN] = v;
t.d -= 4503599627370496.0;
r = (t.u[BYTE_ORDER==LITTLE_ENDIAN] &gt;&gt; 20) - 0x3FF;
</PRE>The code above loads a 64-bit (IEEE-754 floating-point) double with a 32 
bit integer by storing the integer in the mantissa while the exponent is set to 
2<SUP>52</SUP>. From this newly minted double, 2<SUP>52</SUP> (expressed as a 
double) is subtracted, which sets the resulting exponent to the log base 2 of 
the input value, v. All that is left is shifting the exponent bits into position 
(20 bits right) and subtracting the bias, 0x3FF (which is 1023 decimal). This 
technique only takes 5 operations, but many CPUs are slow at manipulating 
doubles, and the endianess of the architecture must be accommodated. 
<P>Eric Cole sent me this on January 15, 2006. Evan Felix pointed out a typo on 
April 4, 2006. 
<P>
<HR>

<H3><A name=IntegerLogLookup>Find the log base 2 of an integer with a lookup 
table </A></H3><PRE>static const char LogTable256[] = 
{
  0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,
  4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
  5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
  5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
  6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
  6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
  6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
  6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7
};

unsigned int v; // 32-bit word to find the log of
unsigned r;     // r will be lg(v)
register unsigned int t, tt; // temporaries

if (tt = v &gt;&gt; 16)
{
  r = (t = tt &gt;&gt; 8) ? 24 + LogTable256[t] : 16 + LogTable256[tt];
}
else 
{
  r = (t = v &gt;&gt; 8) ? 8 + LogTable256[t] : LogTable256[v];
}

</PRE>The lookup table method takes only about 7 operations to find the log of a 
32-bit value. If extended for 64-bit quantities, it would take roughly 9 
operations. Another operation can be trimmed off by using four tables, with the 
possible additions incorporated into each. Using int table elements may be 
faster, depending on your architecture. 
<P><PRE>// To initially generate the log table algorithmically:
LogTable256[0] = LogTable256[1] = 0;
for (int i = 2; i &lt; 256; i++) 
{
  LogTable256[i] = 1 + LogTable256[i / 2];
}
</PRE>Behdad Esfahbod and I shaved off a fraction of an operation (on average) 
on May 18, 2005. Yet another fraction of an operation was removed on November 
14, 2006 by Emanuel Hoogeveen. 
<HR>

<H3><A name=IntegerLog>Find the log base 2 of an N-bit integer in O(lg(N)) 
operations </A></H3><PRE>unsigned int v;  // 32-bit value to find the log2 of 
const unsigned int b[] = {0x2, 0xC, 0xF0, 0xFF00, 0xFFFF0000};
const unsigned int S[] = {1, 2, 4, 8, 16};
int i;

register unsigned int r = 0; // result of log2(v) will go here
for (i = 4; i &gt;= 0; i--) // unroll for speed...
{
  if (v &amp; b[i])
  {
    v &gt;&gt;= S[i];
    r |= S[i];
  } 
}

// OR (IF YOUR CPU BRANCHES SLOWLY):

register unsigned int shift;
shift = ( ( v &amp; 0xFFFF0000 ) != 0 ) &lt;&lt; 4; v &gt;&gt;= shift; r |= shift;
shift = ( ( v &amp; 0xFF00     ) != 0 ) &lt;&lt; 3; v &gt;&gt;= shift; r |= shift;
shift = ( ( v &amp; 0xF0       ) != 0 ) &lt;&lt; 2; v &gt;&gt;= shift; r |= shift;
shift = ( ( v &amp; 0xC        ) != 0 ) &lt;&lt; 1; v &gt;&gt;= shift; r |= shift;
shift = ( ( v &amp; 0x2        ) != 0 ) &lt;&lt; 0; v &gt;&gt;= shift; r |= shift;

// OR (IF YOU KNOW v IS A POWER OF 2):

static const unsigned int b[] = {0xAAAAAAAA, 0xCCCCCCCC, 0xF0F0F0F0, 
                                 0xFF00FF00, 0xFFFF0000};
register unsigned int r = (v &amp; b[0]) != 0;
for (i = 4; i &gt; 0; i--) // unroll for speed...
{
  r |= ((v &amp; b[i]) != 0) &lt;&lt; i;
}
</PRE>Of course, to extend the code to find the log of a 33- to 64-bit number, 
we would append another element, 0xFFFFFFFF00000000, to b, append 32 to S, and 
loop from 5 to 0. This method is much slower than the earlier table-lookup 
version, but if you don't want big table or your architecture is slow to access 
memory, it's a good choice. The second variation involves more operations, but 
it may be faster on machines with high branch costs (e.g. PowerPC), and it was 
sent to me by <A href="http://www.balance-software.com/ec/">Eric Cole</A> on 
January 7, 2006. The third variation was suggested to me by <A 
href="http://www.ece.ucdavis.edu/~jowens/">John Owens</A> on April 24, 2002; 
it's faster, but <EM>it is only suitable when the input is known to be a power 
of 2</EM>. On May 25, 2003, Ken Raeburn suggested improving the general case by 
using smaller numbers for b[], which load faster on some architectures (for 
instance if the word size is 16 bits, then only one load instruction may be 
needed). These values work for the general version, but not for the special-case 
version below it, where v is a power of 2; Glenn Slayden brought this oversight 
to my attention on December 12, 2003. 
<P>
<HR>

<H3><A name=IntegerLogDeBruijn>Find the log base 2 of an N-bit integer in 
O(lg(N)) operations with multiply and lookup </A></H3><PRE>unsigned int v; // find the log base 2 of 32-bit v
int r;          // result goes here

static const int MultiplyDeBruijnBitPosition[32] = 
{
  0, 1, 28, 2, 29, 14, 24, 3, 30, 22, 20, 15, 25, 17, 4, 8, 
  31, 27, 13, 23, 21, 19, 16, 7, 26, 12, 18, 6, 11, 5, 10, 9
};

v |= v &gt;&gt; 1; // first round down to power of 2 
v |= v &gt;&gt; 2;
v |= v &gt;&gt; 4;
v |= v &gt;&gt; 8;
v |= v &gt;&gt; 16;
v = (v &gt;&gt; 1) + 1;

r = MultiplyDeBruijnBitPosition[(v * 0x077CB531UL) &gt;&gt; 27];
</PRE>The code above computes the log base 2 of a 32-bit integer with a small 
table lookup and multiply. It requires only 15 operations, compared to (up to) 
20 for the previous method. The purely table-based method requires the fewest 
operations, but this offers a reasonable compromise between table size and 
speed. If v is known to be a power of 2, then only the last line is needed (3 
operations). 
<P>Eric Cole devised this January 8, 2006 after reading about the entry below to 
<A 
href="http://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2">round 
up to a power of 2</A> and the method below for <A 
href="http://graphics.stanford.edu/~seander/bithacks.html#ZerosOnRightMultLookup">computing 
the number of trailing bits with a multiply and lookup</A> using a DeBruijn 
sequence. 
<P>
<HR>

<H3><A name=IntegerLog10>Find integer log base 10 of an integer </A></H3><PRE>unsigned int v; // non-zero 32-bit integer value to compute the log base 10 of 
int r;          // result goes here
int t;          // temporary

static unsigned int const PowersOf10[] = 
    {1, 10, 100, 1000, 10000, 100000,
     1000000, 10000000, 100000000, 1000000000};

t = (IntegerLogBase2(v) + 1) * 1233 &gt;&gt; 12; // (use a lg2 method from above)
r = t - (v &lt; PowersOf10[t]);
</PRE>The integer log base 10 is computed by first using one of the techniques 
above for finding the log base 2. By the relationship log<SUB>10</SUB>(v) = 
log<SUB>2</SUB>(v) / log<SUB>2</SUB>(10), we need to multiply it by 
1/log<SUB>2</SUB>(10), which is approximately 1233/4096, or 1233 followed by a 
right shift of 12. Adding one is needed because the IntegerLogBase2 rounds down. 
Finally, since the value t is only an approximation that may be off by one, the 
exact value is found by subtracting the result of v &lt; PowersOf10[t]. 
<P>This method takes 6 more operations than IntegerLogBase2. It may be sped up 
(on machines with fast memory access) by modifying the log base 2 table-lookup 
method above so that the entries hold what is computed for t (that is, pre-add, 
-mulitply, and -shift). Doing so would require a total of only 9 operations to 
find the log base 10, assuming 4 tables were used (one for each byte of v). 
<P>Eric Cole suggested I add a version of this on January 7, 2006. 
<P>
<HR>

<H3><A name=IntegerLog10Obvious>Find integer log base 10 of an integer the 
obvious way </A></H3><PRE>unsigned int v; // non-zero 32-bit integer value to compute the log base 10 of 
int r;          // result goes here

r = (v &gt;= 1000000000) ? 9 : (v &gt;= 100000000) ? 8 : (v &gt;= 10000000) ? 7 : 
    (v &gt;= 1000000) ? 6 : (v &gt;= 100000) ? 5 : (v &gt;= 10000) ? 4 : 
    (v &gt;= 1000) ? 3 : (v &gt;= 100) ? 2 : (v &gt;= 10) ? 1 : 0;
</PRE>This method works well when the input is uniformly distributed over 32-bit 
values because 76% of the inputs are caught by the first compare, 21% are caught 
by the second compare, 2% are caught by the third, and so on (chopping the 
remaining down by 90% with each comparision). As a result, less than 2.6 
operations are needed on average. 
<P>On April 18, 2007, Emanuel Hoogeveen suggested a variation on this where the 
conditions used divisions, which were not as fast as simple comparisons. 
<HR>

<H3><A name=IntegerLogFloat>Find integer log base 2 of a 32-bit IEEE float 
</A></H3><PRE>const float v; // find int(log2(v)), where v &gt; 0.0 &amp;&amp; finite(v) &amp;&amp; isnormal(v)
int c;         // 32-bit int c gets the result;

c = *(const int *) &amp;v;  // OR, for portability:  memcpy(&amp;c, &amp;v, sizeof c);
c = (c &gt;&gt; 23) - 127;
</PRE>The above is fast, but IEEE 754-compliant architectures utilize 
<EM>subnormal</EM> (also called <EM>denormal</EM>) floating point numbers. These 
have the exponent bits set to zero (signifying pow(2,-127)), and the mantissa is 
not normalized, so it contains leading zeros and thus the log2 must be computed 
from the mantissa. To accomodate for subnormal numbers, use the following: <PRE>const float v;              // find int(log2(v)), where v &gt; 0.0 &amp;&amp; finite(v)
int c;                      // 32-bit int c gets the result;
int x = *(const int *) &amp;v;  // OR, for portability:  memcpy(&amp;x, &amp;v, sizeof x);

c = x &gt;&gt; 23;          

if (c)
{
  c -= 127;
}
else
{ // subnormal, so recompute using mantissa: c = intlog2(x) - 149;
  register unsigned int t; // temporary
  // Note that LogTable256 was defined <A href="http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogLookup">earlier</A>
  if (t = x &gt;&gt; 16)
  {
    c = LogTable256[t] - 133;
  }
  else
  {
    c = (t = x &gt;&gt; 8) ? LogTable256[t] - 141 : LogTable256[x] - 149;
  }
}
</PRE>On June 20, 2004, Sean A. Irvine suggested that I include code to handle 
subnormal numbers. On June 11, 2005, Falk Hüffner pointed out that ISO C99 6.5/7 
specified undefined behavior for the common type punning idiom *(int *)&amp;, 
though it has worked on 99.9% of C compilers. He proposed using memcpy for 
maximum portability or a union with a float and an int for better code 
generation than memcpy on some compilers. 
<P>
<HR>

<H3><A name=IntegerLogRootFloat>Find integer log base 2 of the pow(2, r)-root of 
a 32-bit IEEE float (for unsigned integer r) </A></H3><PRE>const int r;
const float v; // find int(log2(pow((double) v, 1. / pow(2, r)))), 
               // where isnormal(v) and v &gt; 0
int c;         // 32-bit int c gets the result;

c = *(const int *) &amp;v;  // OR, for portability:  memcpy(&amp;c, &amp;v, sizeof c);
c = ((((c - 0x3f800000) &gt;&gt; r) + 0x3f800000) &gt;&gt; 23) - 127;
</PRE>So, if r is 0, for example, we have c = int(log2((double) v)). If r is 1, 
then we have c = int(log2(sqrt((double) v))). If r is 2, then we have c = 
int(log2(pow((double) v, 1./4))). 
<P>On June 11, 2005, Falk Hüffner pointed out that ISO C99 6.5/7 left the type 
punning idiom *(int *)&amp; undefined, and he suggested using memcpy. 
<P>
<HR>

<H3><A name=ZerosOnRightLinear>Count the consecutive zero bits (trailing) on the 
right linearly </A></H3><PRE>int v;  // input to count trailing zero bits
int c;  // output: c will count v's trailing zero bits,
        // so if v is 1101000 (base 2), then c will be 3
if (v)
{
  v = (v ^ (v - 1)) &gt;&gt; 1;  // Set v's trailing 0s to 1s and zero rest
  for (c = 0; v; c++)
  {
    v &gt;&gt;= 1;
  }
}
else
{
  c = CHAR_BIT * sizeof(v);
}
</PRE>The average number of trailing zero bits in a (uniformly distributed) 
random binary number is one, so this O(trailing zeros) solution isn't that bad 
compared to the faster methods below. 
<P>Jim Cole suggested I add a linear-time method for counting the trailing zeros 
on August 15, 2007. 
<HR>

<H3><A name=ZerosOnRightParallel>Count the consecutive zero bits (trailing) on 
the right in parallel </A></H3><PRE>unsigned int v;      // 32-bit word input to count zero bits on right
unsigned int c = 32; // c will be the number of zero bits on the right

static const unsigned int B[] = {0x55555555, 0x33333333, 0x0F0F0F0F, 0x00FF00FF, 0x0000FFFF};
static const unsigned int S[] = {1, 2, 4, 8, 16}; // Our Magic Binary Numbers

for (int i = 4; i &gt;= 0; --i) // unroll for more speed
{
  if (v &amp; B[i])
  {
    v &lt;&lt;= S[i];
    c -= S[i];
  }
}

if (v)
{
  c--;
}
</PRE>Here, we are basically doing the same operations as finding the log base 2 
in parallel, but the values of b are inverted (in order to count from the right 
rather than the left), we shift v up rather than down, and c starts at the 
maximum and is decreased. We also have the additional step at the end, 
decrementing c if there is anything left in v. The number of operations is at 
most 4 * lg(N) + 2, roughly, for N bit words. 
<P>
<HR>

<H3><A name=ZerosOnRightBinSearch>Count the consecutive zero bits (trailing) on 
the right by binary search </A></H3><PRE>unsigned int v;     // 32-bit word input to count zero bits on right
unsigned int c = 0; // c will be the number of zero bits on the right,
                    // so if v is 1101000 (base 2), then c will be 3
// NOTE: if 0 == v, then c = 31.
if (v &amp; 0x1) 
{
  // special case for odd v (assumed to happen half of the time)
}
else
{
  if ((v &amp; 0xffff) == 0) 
  {  
    v &gt;&gt;= 16;  
    c += 16;
  }
  if ((v &amp; 0xff) == 0) 
  {  
    v &gt;&gt;= 8;  
    c += 8;
  }
  if ((v &amp; 0xf) == 0) 
  {  
    v &gt;&gt;= 4;
    c += 4;
  }
  if ((v &amp; 0x3) == 0) 
  {  
    v &gt;&gt;= 2;
    c += 2;
  }
  if ((v &amp; 0x1) == 0)
  {
    c++;
  }
}	
</PRE>The code above is similar to the previous method, but it computes the 
number of trailing zeros by accumulating c in a manner akin to binary search. In 
the first step, it checks if the bottom 16 bits of v are zeros, and if so, 
shifts v right 16 bits and adds 16 to c, which reduces the number of bits in v 
to consider by half. Each of the subsequent conditional steps likewise halves 
the number of bits until there is only 1. This method is faster than the last 
one (by about 33%) because the bodies of the if statements are executed less 
often. 
<P>Matt Whitlock suggested this on January 25, 2006. 
<P>
<HR>

<H3><A name=ZerosOnRightFloatCast>Count the consecutive zero bits (trailing) on 
the right by casting to a float </A></H3><PRE>unsigned int v;            // find the number of trailing zeros in v
int r;                     // the result goes here
float f = (float)(v &amp; -v); // cast the least significant bit in v to a float
r = (*(unsigned int *)&amp;f &gt;&gt; 23) - 0x7f;
</PRE>Although this only takes about 6 operations, the time to convert an 
integer to a float can be high on some machines. The exponent of the 32-bit IEEE 
floating point representation is shifted down, and the bias is subtracted to 
give the position of the least significant 1 bit set in v. If v is zero, then 
the result is -127. 
<P>
<HR>

<H3><A name=ZerosOnRightModLookup>Count the consecutive zero bits (trailing) on 
the right with modulus division and lookup </A></H3><PRE>unsigned int v;  // find the number of trailing zeros in v
int r;           // put the result in r
static const int Mod37BitPosition[] = // map a bit value mod 37 to its position
{
  32, 0, 1, 26, 2, 23, 27, 0, 3, 16, 24, 30, 28, 11, 0, 13, 4,
  7, 17, 0, 25, 22, 31, 15, 29, 10, 12, 6, 0, 21, 14, 9, 5,
  20, 8, 19, 18
};
r = Mod37BitPosition[(-v &amp; v) % 37];
</PRE>The code above finds the number of zeros that are trailing on the right, 
so binary 0100 would produce 2. It makes use of the fact that the first 32 bit 
position values are relatively prime with 37, so performing a modulus division 
with 37 gives a unique number from 0 to 36 for each. These numbers may then be 
mapped to the number of zeros using a small lookup table. It uses only 4 
operations, however indexing into a table and performing modulus division may 
make it unsuitable for some situations. I came up with this independently and 
then searched for a subsequence of the table values, and found it was invented 
earlier by Reiser, according to <A 
href="http://www.hackersdelight.org/HDcode/ntz.cc">Hacker's Delight</A>. 
<P>
<HR>

<H3><A name=ZerosOnRightMultLookup>Count the consecutive zero bits (trailing) on 
the right with multiply and lookup </A></H3><PRE>unsigned int v;  // find the number of trailing zeros in 32-bit v 
int r;           // result goes here
static const int MultiplyDeBruijnBitPosition[32] = 
{
  0, 1, 28, 2, 29, 14, 24, 3, 30, 22, 20, 15, 25, 17, 4, 8, 
  31, 27, 13, 23, 21, 19, 16, 7, 26, 12, 18, 6, 11, 5, 10, 9
};
r = MultiplyDeBruijnBitPosition[((v &amp; -v) * 0x077CB531UL) &gt;&gt; 27];
</PRE>Converting bit vectors to indices of set bits is an example use for this. 
It requires one more operation than the earlier one involving modulus division, 
but the multiply may be faster. The expression (v &amp; -v) extracts the least 
significant 1 bit from v. The constant 0x077CB531UL is a de Bruijn sequence, 
which produces a unique pattern of bits into the high 5 bits for each possible 
bit position that it is multiplied against. When there are no bits set, it 
returns 0. More information can be found by reading the paper <A 
href="http://citeseer.ist.psu.edu/leiserson98using.html">Using de Bruijn 
Sequences to Index 1 in a Computer Word</A> by Charles E. Leiserson, Harald 
Prokof, and Keith H. Randall. 
<P>On October 8, 2005 <A href="http://onezero.org/">Andrew Shapira</A> suggested 
I add this. 
<P>
<HR>

<H3><A name=RoundUpPowerOf2Float>Round up to the next highest power of 2 by 
float casting </A></H3><PRE>unsigned int const v; // Round this 32-bit value to the next highest power of 2
unsigned int r;       // Put the result here. (So v=3 -&gt; r=4; v=8 -&gt; r=8)

if (v &gt; 1) 
{
  float f = (float)v;
  unsigned int const t = 1 &lt;&lt; ((*(unsigned int *)&amp;f &gt;&gt; 23) - 0x7f);
  r = t &lt;&lt; (t &lt; v);
}
else 
{
  r = 1;
}
</PRE>The code above uses 8 operations, but works on all v &lt;= (1&lt;&lt;31). 
<P>Quick and dirty version, for domain of 1 &lt; v &lt; (1&lt;&lt;25): <PRE>float f = (float)(v - 1);  
r = 1 &lt;&lt; ((*(unsigned int*)(&amp;f) &gt;&gt; 23) - 126);
</PRE>Although the quick and dirty version only uses around 6 operations, it is 
roughly three times slower than the <A 
href="http://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2">technique 
below</A> (which involves 12 operations) when benchmarked on an Athlon™ XP 2100+ 
CPU. Some CPUs will fare better with it, though. 
<P>On September 27, 2005 Andi Smithers suggested I include a technique for 
casting to floats to find the lg of a number for rounding up to a power of 2. 
Similar to the quick and dirty version here, his version worked with values less 
than (1&lt;&lt;25), due to mantissa rounding, but it used one more operation. 
<P>
<HR>

<H3><A name=RoundUpPowerOf2>Round up to the next highest power of 2 </A></H3><PRE>unsigned int v; // compute the next highest power of 2 of 32-bit v

v--;
v |= v &gt;&gt; 1;
v |= v &gt;&gt; 2;
v |= v &gt;&gt; 4;
v |= v &gt;&gt; 8;
v |= v &gt;&gt; 16;
v++;
</PRE>In 12 operations, this code computes the next highest power of 2 for a 
32-bit integer. The result may be expressed by the formula 1 &lt;&lt; (lg(v - 1) 
+ 1). Note that in the edge case where v is 0, it returns 0, which isn't a power 
of 2; you might append the expression v += (v == 0) to remedy this if it 
matters. It would be faster by 2 operations to use the formula and the log base 
2 methed that uses a lookup table, but in some situations, lookup tables are not 
suitable, so the above code may be best. (On a Athlon™ XP 2100+ I've found the 
above shift-left and then OR code is as fast as using a single BSR assembly 
language instruction, which scans in reverse to find the highest set bit.) It 
works by copying the highest set bit to all of the lower bits, and then adding 
one, which results in carries that set all of the lower bits to 0 and one bit 
beyond the highest set bit to 1. If the original number was a power of 2, then 
the decrement will reduce it to one less, so that we round up to the same 
original value. 
<P>Devised by Sean Anderson, Sepember 14, 2001. Pete Hart pointed me to <A 
href="http://groups.google.com/group/comp.lang.python/browse_thread/thread/c4d3aae0df917df5/6fdae3872f9de79d?lnk=st&amp;q=comp.lang.python+zeddy&amp;rnum=6#6fdae3872f9de79d">a 
couple newsgroup posts</A> by him and William Lewis in February of 1997, where 
they arrive at the same algorithm. 
<P>
<HR>

<H3><A name=InterleaveTableObvious>Interleave bits the obvious way </A></H3><PRE>unsigned short x;   // Interleave bits of x and y, so that all of the
unsigned short y;   // bits of x are in the even positions and y in the odd;
unsigned int z = 0; // z gets the resulting Morton Number.

for (int i = 0; i &lt; sizeof(x) * CHAR_BIT; i++) // unroll for more speed...
{
  z |= (x &amp; 1 &lt;&lt; i) &lt;&lt; i | (y &amp; 1 &lt;&lt; i) &lt;&lt; (i + 1);
}
</PRE>Interleaved bits (aka Morton numbers) are useful for linearizing 2D 
integer coordinates, so x and y are combined into a single number that can be 
compared easily and has the property that a number is usually close to another 
if their x and y values are close. 
<P>
<HR>

<H3><A name=InterleaveTableLookup>Interleave bits by table lookup </A></H3><PRE>static const unsigned short MortonTable256[] = 
{
  0x0000, 0x0001, 0x0004, 0x0005, 0x0010, 0x0011, 0x0014, 0x0015, 
  0x0040, 0x0041, 0x0044, 0x0045, 0x0050, 0x0051, 0x0054, 0x0055, 
  0x0100, 0x0101, 0x0104, 0x0105, 0x0110, 0x0111, 0x0114, 0x0115, 
  0x0140, 0x0141, 0x0144, 0x0145, 0x0150, 0x0151, 0x0154, 0x0155, 
  0x0400, 0x0401, 0x0404, 0x0405, 0x0410, 0x0411, 0x0414, 0x0415, 
  0x0440, 0x0441, 0x0444, 0x0445, 0x0450, 0x0451, 0x0454, 0x0455, 
  0x0500, 0x0501, 0x0504, 0x0505, 0x0510, 0x0511, 0x0514, 0x0515, 
  0x0540, 0x0541, 0x0544, 0x0545, 0x0550, 0x0551, 0x0554, 0x0555, 
  0x1000, 0x1001, 0x1004, 0x1005, 0x1010, 0x1011, 0x1014, 0x1015, 
  0x1040, 0x1041, 0x1044, 0x1045, 0x1050, 0x1051, 0x1054, 0x1055, 
  0x1100, 0x1101, 0x1104, 0x1105, 0x1110, 0x1111, 0x1114, 0x1115, 
  0x1140, 0x1141, 0x1144, 0x1145, 0x1150, 0x1151, 0x1154, 0x1155, 
  0x1400, 0x1401, 0x1404, 0x1405, 0x1410, 0x1411, 0x1414, 0x1415, 
  0x1440, 0x1441, 0x1444, 0x1445, 0x1450, 0x1451, 0x1454, 0x1455, 
  0x1500, 0x1501, 0x1504, 0x1505, 0x1510, 0x1511, 0x1514, 0x1515, 
  0x1540, 0x1541, 0x1544, 0x1545, 0x1550, 0x1551, 0x1554, 0x1555, 
  0x4000, 0x4001, 0x4004, 0x4005, 0x4010, 0x4011, 0x4014, 0x4015, 
  0x4040, 0x4041, 0x4044, 0x4045, 0x4050, 0x4051, 0x4054, 0x4055, 
  0x4100, 0x4101, 0x4104, 0x4105, 0x4110, 0x4111, 0x4114, 0x4115, 
  0x4140, 0x4141, 0x4144, 0x4145, 0x4150, 0x4151, 0x4154, 0x4155, 
  0x4400, 0x4401, 0x4404, 0x4405, 0x4410, 0x4411, 0x4414, 0x4415, 
  0x4440, 0x4441, 0x4444, 0x4445, 0x4450, 0x4451, 0x4454, 0x4455, 
  0x4500, 0x4501, 0x4504, 0x4505, 0x4510, 0x4511, 0x4514, 0x4515, 
  0x4540, 0x4541, 0x4544, 0x4545, 0x4550, 0x4551, 0x4554, 0x4555, 
  0x5000, 0x5001, 0x5004, 0x5005, 0x5010, 0x5011, 0x5014, 0x5015, 
  0x5040, 0x5041, 0x5044, 0x5045, 0x5050, 0x5051, 0x5054, 0x5055, 
  0x5100, 0x5101, 0x5104, 0x5105, 0x5110, 0x5111, 0x5114, 0x5115, 
  0x5140, 0x5141, 0x5144, 0x5145, 0x5150, 0x5151, 0x5154, 0x5155, 
  0x5400, 0x5401, 0x5404, 0x5405, 0x5410, 0x5411, 0x5414, 0x5415, 
  0x5440, 0x5441, 0x5444, 0x5445, 0x5450, 0x5451, 0x5454, 0x5455, 
  0x5500, 0x5501, 0x5504, 0x5505, 0x5510, 0x5511, 0x5514, 0x5515, 
  0x5540, 0x5541, 0x5544, 0x5545, 0x5550, 0x5551, 0x5554, 0x5555
};

unsigned short x; // Interleave bits of x and y, so that all of the
unsigned short y; // bits of x are in the even positions and y in the odd;
unsigned int z;   // z gets the resulting 32-bit Morton Number.

z = MortonTable256[y &gt;&gt; 8]   &lt;&lt; 17 | 
    MortonTable256[x &gt;&gt; 8]   &lt;&lt; 16 |
    MortonTable256[y &amp; 0xFF] &lt;&lt;  1 | 
    MortonTable256[x &amp; 0xFF];

</PRE>For more speed, use an additional table with values that are 
MortonTable256 pre-shifted one bit to the left. This second table could then be 
used for the y lookups, thus reducing the operations by two, but almost doubling 
the memory required. Extending this same idea, four tables could be used, with 
two of them pre-shifted by 16 to the left of the previous two, so that we would 
only need 11 operations total. 
<HR>

<H3><A name=Interleave64bitOps>Interleave bits with 64-bit multiply</A> </H3>In 
11 operations, this version interleaves bits of two bytes (rather than shorts, 
as in the other versions), but many of the operations are 64-bit multiplies so 
it isn't appropriate for all machines. <PRE>unsigned char x;  // Interleave bits of (8-bit) x and y, so that all of the
unsigned char y;  // bits of x are in the even positions and y in the odd;
unsigned short z; // z gets the resulting 16-bit Morton Number.

z = ((x * 0x0101010101010101ULL &amp; 0x8040201008040201ULL) * 
     0x0102040810204081ULL &gt;&gt; 49) &amp; 0x5555 |
    ((y * 0x0101010101010101ULL &amp; 0x8040201008040201ULL) * 
     0x0102040810204081ULL &gt;&gt; 48) &amp; 0xAAAA;
</PRE>Holger Bettag was inspired to suggest this technique on October 10, 2004 
after reading the multiply-based bit reversals here. 
<P>
<HR>

<H3><A name=InterleaveBMN>Interleave bits by Binary Magic Numbers </A></H3><PRE>static const unsigned int B[] = {0x55555555, 0x33333333, 0x0F0F0F0F, 0x00FF00FF};
static const unsigned int S[] = {1, 2, 4, 8};

unsigned int x; // Interleave lower 16 bits of x and y, so the bits of x
unsigned int y; // are in the even positions and bits from y in the odd;
unsigned int z; // z gets the resulting 32-bit Morton Number.

x = (x | (x &lt;&lt; S[3])) &amp; B[3];
x = (x | (x &lt;&lt; S[2])) &amp; B[2];
x = (x | (x &lt;&lt; S[1])) &amp; B[1];
x = (x | (x &lt;&lt; S[0])) &amp; B[0];

y = (y | (y &lt;&lt; S[3])) &amp; B[3];
y = (y | (y &lt;&lt; S[2])) &amp; B[2];
y = (y | (y &lt;&lt; S[1])) &amp; B[1];
y = (y | (y &lt;&lt; S[0])) &amp; B[0];

z = x | (y &lt;&lt; 1);
</PRE>
<HR>

<H3><A name=ZeroInWord>Determine if a word has a zero byte </A></H3><PRE>// Fewer operations:
unsigned int v; // 32-bit word to check if any 8-bit byte in it is 0
bool hasZeroByte = ~((((v &amp; 0x7F7F7F7F) + 0x7F7F7F7F) | v) | 0x7F7F7F7F);
</PRE>The code above may be useful when doing a fast string copy in which a word 
is copied at a time; it uses 5 operations. On the other hand, testing for a null 
byte in the obvious ways (which follow) have at least 7 operations (when counted 
in the most sparing way), and at most 12. <PRE>// More operations:
bool hasNoZeroByte = ((v &amp; 0xff) &amp;&amp; (v &amp; 0xff00) &amp;&amp; (v &amp; 0xff0000) &amp;&amp; (v &amp; 0xff000000))
// OR:
unsigned char * p = (unsigned char *) &amp;v;  
bool hasNoZeroByte = *p &amp;&amp; *(p + 1) &amp;&amp; *(p + 2) &amp;&amp; *(p + 3);
</PRE>The code at the beginning of this section (labeled "Fewer operations") 
works by first zeroing the high bits of the 4 bytes in the word. Subsequently, 
it adds a number that will result in an overflow to the high bit of a byte if 
any of the low bits were initialy set. Next the high bits of the original word 
are ORed with these values; thus, the high bit of a byte is set iff any bit in 
the byte was set. Finally, we determine if any of these high bits are zero by 
ORing with ones everywhere except the high bits and inverting the result. 
Extending to 64 bits is trivial; simply increase the constants to be 
0x7F7F7F7F7F7F7F7F. 
<P>For an additional improvement, a fast pretest that requires only 4 operations 
may be performed to determine if the word <EM>may</EM> have a zero byte. The 
test also returns true if the high byte is 0x80, so there are occasional false 
positives, but the slower and more reliable version above may then be used on 
candidates for an overall increase in speed with correct output. 
<P><PRE>bool hasZeroByte = ((v + 0x7efefeff) ^ ~v) &amp; 0x81010100;
if (hasZeroByte) // or may just have 0x80 in the high byte
{
  hasZeroByte = ~((((v &amp; 0x7F7F7F7F) + 0x7F7F7F7F) | v) | 0x7F7F7F7F);
}
</PRE>
<P>There is yet a faster method — use <A 
href="http://graphics.stanford.edu/~seander/bithacks.html#HasLessInWord"><CODE>hasless</CODE></A>(v, 
1), which is defined below; it works in 4 operations and requires no subsquent 
verification. It simplifies to <PRE>bool hasZeroByte = (v - 0x01010101UL) &amp; ~v &amp; 0x80808080UL;</PRE>The 
subexpression (v - 0x01010101UL), evaluates to a high bit set in any byte 
whenever the corresponding byte in v is zero or greater than 0x80. The 
sub-expression ~v &amp; 0x80808080UL evaluates to high bits set in bytes where 
the byte of v doesn't have its high bit set (so the byte was less than 0x80). 
Finally, by ANDing these two sub-expressions the result is the high bits set 
where the bytes in v were zero, since the high bits set due to a value greater 
than 0x80 in the first sub-expression are masked off by the second. 
<P>Paul Messmer suggested the fast pretest improvement on October 2, 2004. Juha 
Järvi later suggested hasless(v, 1) on April 6, 2005, which he found on <A 
href="http://www.azillionmonkeys.com/qed/asmexample.html">Paul Hsieh's Assembly 
Lab</A>; previously it was written in a newsgroup post on April 27, 1987 by Alan 
Mycroft. 
<P>
<HR>

<H3><A name=HasLessInWord>Determine if a word has a byte less than n 
</A></H3>Test if a word x contains an unsigned byte with value &lt; n. 
Specifically for n=1, it can be used to find a 0-byte by examining one long at a 
time, or any byte by XORing x with a mask first. Uses 4 arithmetic/logical 
operations when n is constant. 
<P>Requirements: x&gt;=0; 0&lt;=n&lt;=128 <PRE>#define hasless(x,n) (((x)-~0UL/255*(n))&amp;~(x)&amp;~0UL/255*128)
</PRE>To count the number of bytes in x that are less than n in 7 operations, 
use <PRE>#define countless(x,n) \
(((~0UL/255*(127+(n))-((x)&amp;~0UL/255*127))&amp;~(x)&amp;~0UL/255*128)/128%255)
</PRE>
<P>Juha Järvi sent this clever technique to me on April 6, 2005. The 
<CODE>countless</CODE> macro was added by Sean Anderson on April 10, 2005, 
inspired by Juha's <CODE>countmore</CODE>, below. 
<HR>

<H3><A name=HasMoreInWord>Determine if a word has a byte greater than n 
</A></H3>Test if a word x contains an unsigned byte with value &gt; n. Uses 3 
arithmetic/logical operations when n is constant. 
<P>Requirements: x&gt;=0; 0&lt;=n&lt;=127 <PRE>#define hasmore(x,n) (((x)+~0UL/255*(127-(n))|(x))&amp;~0UL/255*128)
</PRE>To count the number of bytes in x that are more than n in 6 operations, 
use: <PRE>#define countmore(x,n) \
(((((x)&amp;~0UL/255*127)+~0UL/255*(127-(n))|(x))&amp;~0UL/255*128)/128%255)
</PRE>
<P>The macro <CODE>hasmore</CODE> was suggested by Juha Järvi on April 6, 2005, 
and he added <CODE>countmore</CODE> on April 8, 2005. 
<HR>

<H3><A name=HasBetweenInWord>Determine if a word has a byte between m and n 
</A></H3>When m&nbsp;&lt;&nbsp;n, this technique tests if a word x contains an 
unsigned byte value, such that m &lt; value &lt; n. <!--When m&nbsp;&gt;&nbsp;n, it tests for byte values
outside the range; that is value < n and m <= value.-->It 
uses 7 arithmetic/logical operations when n and m are constant. 
<P>Note: Bytes that equal n can be reported by <CODE>likelyhasbetween</CODE> as 
false positives, so this should be checked by character if a certain result is 
needed. 
<P>Requirements: x&gt;=0; 0&lt;=m&lt;=127; 0&lt;=n&lt;=128 
<P <pre>#define likelyhasbetween(x,m,n) \ 
((((x)-~0UL/255*(n))&amp;~(x)&amp;((x)&amp;~0UL/255*127)+~0UL/255*(127-(m)))&amp;~0UL/255*128) 
<PRE></PRE>This technique would be suitable for a fast pretest. A variation that 
takes one more operation (8 total for constant m and n) but provides the exact 
answer is: <PRE>#define hasbetween(x,m,n) \
((~0UL/255*(127+(n))-((x)&amp;~0UL/255*127)&amp;~(x)&amp;((x)&amp;~0UL/255*127)+~0UL/255*(127-m))&amp;~0UL/255*128)
</PRE>To count the number of bytes in x that are between m and n (exclusive) in 
10 operations, use: <PRE>#define countbetween(x,m,n) (hasbetween(x,m,n)/128%255)
</PRE>
<P>Juha Järvi suggested <CODE>likelyhasbetween</CODE> on April 6, 2005. From 
there, Sean Anderson created <CODE>hasbetween</CODE> and 
<CODE>countbetween</CODE> on April 10, 2005. 
<P>
<HR>
<!-- <table>
<tr>
<td><b>Hits:</b></td>
<td><img src="http://www.anderoid.com/cgi-bin/countess.gif?seander2"
 width=56 height=16></td>
</tr>
</table>
--></BODY></HTML>
