# -*- coding: utf-8 -*-
"""Copy of Network science.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qAj_9UOKk4V_BwOLYhg6xBMfggi1eU1r

# Name
# Roll No.
# Network Science

# 1. Adding required files
"""

# Mounts Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
# Set directory to listed path
# %cd /content/drive/My Drive/Dependencies

"""# Cleating Text (removing stop words ,special character e.t.c)"""

import re

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Get English stopwords from NLTK
stop_words = set(stopwords.words('english'))


def clean_text(text):
    # 1. Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)

    # 2. Remove mentions and hashtags
    text = re.sub(r'\@\w+|\#', '', text)

    # 3. Remove special characters and numbers
    text = re.sub(r"[^a-zA-Z\s]", '', text)

    # 4. Convert text to lowercase
    text = text.lower()

    # 5. Tokenize the text
    words = text.split()

    # 6. Remove stopwords using NLTK's stopwords list
    filtered_words = [word for word in words if word not in stop_words]

    # 7. Lemmatize each word
    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]

    # 8. Rejoin the words into cleaned text
    cleaned_text = ' '.join(lemmatized_words)

    return cleaned_text

"""# Counting frequency of word in the given cleaned text"""

def frequency_count(text):
    words = text.split()
    word_frequency = {}

    for word in words:
        if word in word_frequency:
            word_frequency[word] += 1
        else:
            word_frequency[word] = 1

    return word_frequency

"""# Displying frequency of top 50 words in the text"""

import matplotlib.pyplot as plt

def plot_all_word_frequencies(word_counts,chaptercount):
    # Sort the dictionary by frequency in descending order
    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

    # Get the top 20 words and their frequencies
    top_50 = sorted_word_counts[:50]

    # Extract words and frequencies
    words, frequencies = zip(*top_50)

    # Create a vertical bar graph
    plt.figure(figsize=(20, 8))
    plt.bar(words, frequencies, color='skyblue')

    # Add titles and labels
    plt.title(f"Top 50 Word Frequencies from chapter {chaptercount}")
    plt.xlabel('Words')
    plt.ylabel('Frequency')

    # Rotate x-axis labels for readability
    plt.xticks(rotation=45, ha='right')

    # Adjust layout
    plt.tight_layout()
    plt.show()

"""# Graph Visualization function With 20 node for each chapter and edge weight showing common words between those chapter"""

pip install pyvis

import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

def visualize_graph_from_adj_matrix(adj_matrix, node_color='skyblue', edge_color='gray', edge_label_color='red'):

    # Check if input is a square matrix
    if adj_matrix.ndim != 2:
        raise ValueError("Adjacency matrix must be square (N x N).")

    # Create a graph from the adjacency matrix
    G = nx.from_numpy_array(adj_matrix)

    # Set up the figure for visualization with more space
    plt.figure(figsize=(15, 12))  # Increase the figure size for better clarity

    # Generate positions for nodes using Kamada-Kawai layout (better for large graphs)
    pos = nx.kamada_kawai_layout(G, weight='weight')

    # Draw nodes with larger size and black borders for better contrast
    nx.draw_networkx_nodes(
        G,
        pos,
        node_color=node_color,
        node_size=1200,  # Increase node size for better visibility
        edgecolors='black',  # Add a border to nodes
        linewidths=1.5
    )

    # Draw edges with specified color and adjusted transparency for clarity
    nx.draw_networkx_edges(
        G,
        pos,
        edge_color=edge_color,
        width=3,
        alpha=0.7  # Reduce transparency for better edge visibility
    )

    # Draw node labels with larger font size and bold for readability
    nx.draw_networkx_labels(
        G,
        pos,
        font_size=16,
        font_color='black',
        font_weight='bold'
    )

    # Add edge labels (weights) with larger font and red color for distinction
    edge_labels = nx.get_edge_attributes(G, "weight")
    nx.draw_networkx_edge_labels(
        G,
        pos,
        edge_labels=edge_labels,
        font_color=edge_label_color,
        font_size=12,
        font_weight='bold'
    )

    # Add title and remove axes for a cleaner presentation
    plt.title("Graph Representation of the Adjacency Matrix", fontsize=20, fontweight='bold')
    plt.axis('off')

    # Show the plot
    plt.tight_layout()
    plt.show()

"""# Extracting chapter from book and converting it into a text"""

pip install pymupdf

import fitz  # PyMuPDF

def extract_chapter(pdf_path, chapter_start_page, chapter_end_page, output_path):
    """
    Extracts text from a specified range of pages in a PDF file and saves it to a text file.

    Args:
        pdf_path (str): Path to the input PDF file.
        chapter_start_page (int): The starting page number of the chapter (0-based).
        chapter_end_page (int): The ending page number of the chapter (0-based).
        output_path (str): Path to save the extracted text file.
    """
    try:
        # Open the PDF file
        document = fitz.open(pdf_path)
        extracted_text = ""

        # Iterate through the specified page range
        for page_num in range(chapter_start_page, chapter_end_page + 1):
            page = document[page_num]
            extracted_text += page.get_text()

        # Save the extracted text to the output file
        with open(output_path, "w", encoding="utf-8") as output_file:
            output_file.write(extracted_text)

        print(f"Chapter text successfully saved to {output_path}")

    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage:
# extract_chapter("PressmanBook 7th edition.pdf", 95, 118, "chapter4.txt")

extract_chapter("PressmanBook 7th edition.pdf", 30, 55, "ch1.txt")
extract_chapter("PressmanBook 7th edition.pdf", 59, 92, "ch2.txt")
extract_chapter("PressmanBook 7th edition.pdf", 94, 122, "ch3.txt")
extract_chapter("PressmanBook 7th edition.pdf", 124, 145, "ch4.txt")
extract_chapter("PressmanBook 7th edition.pdf", 147, 175, "ch5.txt")
extract_chapter("PressmanBook 7th edition.pdf", 177, 213, "ch6.txt")
extract_chapter("PressmanBook 7th edition.pdf", 215, 243, "ch7.txt")
extract_chapter("PressmanBook 7th edition.pdf", 244, 269, "ch8.txt")
extract_chapter("PressmanBook 7th edition.pdf", 271, 303, "ch9.txt")
extract_chapter("PressmanBook 7th edition.pdf", 305, 340, "ch10.txt")
extract_chapter("PressmanBook 7th edition.pdf", 341, 374, "ch11.txt")
extract_chapter("PressmanBook 7th edition.pdf", 376, 400, "ch12.txt")
extract_chapter("PressmanBook 7th edition.pdf", 402, 423, "ch13.txt")
extract_chapter("PressmanBook 7th edition.pdf", 426, 443, "ch14.txt")
extract_chapter("PressmanBook 7th edition.pdf", 445, 460, "ch15.txt")
extract_chapter("PressmanBook 7th edition.pdf", 461, 476, "ch16.txt")
extract_chapter("PressmanBook 7th edition.pdf", 478, 507, "ch17.txt")
extract_chapter("PressmanBook 7th edition.pdf", 510, 539, "ch18.txt")
extract_chapter("PressmanBook 7th edition.pdf", 540, 556, "ch19.txt")
extract_chapter("PressmanBook 7th edition.pdf", 558, 583, "ch20.txt")

"""# Reading chapters and Printing chapter 1 and chapter 2 data"""

ch1 = open('ch1.txt', encoding='utf-8').read()
ch2 = open('ch2.txt', encoding='utf-8').read()

ch1

ch2

ch3 = open('ch3.txt', encoding='utf-8').read()
ch4 = open('ch4.txt', encoding='utf-8').read()
ch5 = open('ch5.txt', encoding='utf-8').read()
ch6 = open('ch6.txt', encoding='utf-8').read()
ch7 = open('ch7.txt', encoding='utf-8').read()
ch8 = open('ch8.txt', encoding='utf-8').read()
ch9 = open('ch9.txt', encoding='utf-8').read()
ch10 = open('ch10.txt', encoding='utf-8').read()
ch11 = open('ch11.txt', encoding='utf-8').read()
ch12 = open('ch12.txt', encoding='utf-8').read()
ch13 = open('ch13.txt', encoding='utf-8').read()
ch14 = open('ch14.txt', encoding='utf-8').read()
ch15 = open('ch15.txt', encoding='utf-8').read()
ch16 = open('ch16.txt', encoding='utf-8').read()
ch17 = open('ch17.txt', encoding='utf-8').read()
ch18 = open('ch18.txt', encoding='utf-8').read()
ch19 = open('ch19.txt', encoding='utf-8').read()
ch20 = open('ch20.txt', encoding='utf-8').read()

"""# Calling text cleaning function"""

ch1Words = clean_text(ch1)
ch2Words = clean_text(ch2)
ch3Words = clean_text(ch3)
ch4Words = clean_text(ch4)
ch5Words = clean_text(ch5)
ch6Wors = clean_text(ch6)
ch7Words = clean_text(ch7)
ch8Words = clean_text(ch8)
ch9Words = clean_text(ch9)
ch10Words = clean_text(ch10)
ch11Words = clean_text(ch11)
ch12Words = clean_text(ch12)
ch13Words = clean_text(ch13)
ch14Words = clean_text(ch14)
ch15Words = clean_text(ch15)
ch16Words = clean_text(ch16)
ch17Words = clean_text(ch17)
ch18Words = clean_text(ch18)
ch19Words = clean_text(ch19)
ch20Words = clean_text(ch20)

"""# Printing cleaned word from chapter 1 and chapter 2"""

ch1Words

ch2Words

"""# Storing frequency of words for each chapter"""

chapters = [
   ch1Words,
  ch2Words,
  ch3Words,
  ch4Words,
  ch5Words,
  ch6Wors,
  ch7Words,
  ch8Words,
  ch9Words,
  ch10Words,
  ch11Words,
  ch12Words,
  ch13Words,
  ch14Words,
  ch15Words,
  ch16Words,
  ch17Words,
  ch18Words,
  ch19Words,
   ch20Words
]

word_freq_array = []
for i in range(1,21):
  word_freq_array.append(frequency_count(chapters[i-1]))

"""# Printing word frequency for chapter 1"""

word_freq_array[0]

"""# Plotting Word and frequency for each chapter in Bar Graph"""

for i in range(1,21):
  plot_all_word_frequencies(word_freq_array[i-1],i)

"""# Counting common word between two chapters"""

def count_common_words(dict1, dict2):

    # Use set intersection to find common keys (words)
    common_words = set(dict1.keys()) & set(dict2.keys())
    return len(common_words)

"""# Printing common word between chapter 1 and chapter 2"""

count_common_words(word_freq_array[0],word_freq_array[1])

"""# Creating adjcency matrix of common words"""

def create_adjacency_matrix(word_freq_array):

    adjacency_matrix = []

    # Loop over each pair of word-frequency dictionaries and create the matrix
    for i in range(len(word_freq_array)):
        row = []
        for j in range(len(word_freq_array)):
            # Count common words between the i-th and j-th dictionaries
            common_word_count = count_common_words(word_freq_array[i], word_freq_array[j])
            row.append(common_word_count)
        adjacency_matrix.append(row)

    return np.array(adjacency_matrix)

"""# Printing Adjcency matrix"""

adjacency_matrix = create_adjacency_matrix(word_freq_array)
adjacency_matrix

"""# Visualizing common words using Graph Data Structure"""

visualize_graph_from_adj_matrix(np.array(adjacency_matrix))

"""# Visulizing common words using heat map"""

import numpy as np
import plotly.express as px

def plot_interactive_heatmap_with_tooltip(adj_matrix):

    # Number of chapters
    num_chapters = len(adj_matrix)

    # Create the labels for the chapters
    chapters = [f'Ch{i+1}' for i in range(num_chapters)]

    # Create a hover text that will display the common words count
    hover_text = [[f"Common Words: {adj_matrix[i][j]}" for j in range(num_chapters)] for i in range(num_chapters)]

    # Create the heatmap using plotly
    fig = px.imshow(
        adj_matrix,
        labels={'x': 'Chapters', 'y': 'Chapters'},
        x=chapters,
        y=chapters,
        color_continuous_scale='Viridis',  # You can use other color scales
        title="Interactive Heatmap of Common Words Between Chapters",
        text_auto=True  # This will show the text in each cell automatically
    )

    # Add custom hover text
    fig.update_traces(hovertemplate='%{text}')

    # Show the plot
    fig.update_layout(
        xaxis={'side': 'bottom'},  # Move x-axis to bottom for better labeling
        yaxis={'side': 'left'},    # Move y-axis to left for better labeling
        width=800,                # Width of the plot
        height=800,               # Height of the plot
    )
    fig.show()

"""# You can see common word on the cell"""

plot_interactive_heatmap_with_tooltip(adjacency_matrix)

"""# Calculating Clustring coffiecient"""

import networkx as nx

def calculate_clustering_coefficient(adj_matrix):

    # Convert the adjacency matrix to a NetworkX graph using the updated method
    G = nx.from_numpy_array(adj_matrix)

    # Calculate the clustering coefficient for each node
    clustering_coefficients = nx.clustering(G)

    # Calculate the average clustering coefficient for the entire graph
    avg_clustering_coefficient = np.mean(list(clustering_coefficients.values()))

    return avg_clustering_coefficient, clustering_coefficients

avg_cc, node_cc = calculate_clustering_coefficient(adjacency_matrix)
print(f"Average Clustering Coefficient: {avg_cc}")
print("Clustering Coefficients for each node:")
for node, cc in node_cc.items():
    print(f"Node {node+1}: {cc}")

"""# Extracting text for each page"""

import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):

    try:
        # Open the PDF file
        document = fitz.open(pdf_path)
        pages_text = []

        # Iterate through all pages and extract text
        for page_num in range(len(document)):
            page = document.load_page(page_num)  # load_page is more efficient
            pages_text.append(page.get_text())

        # Return the list of extracted text for each page
        return pages_text

    except Exception as e:
        print(f"An error occurred: {e}")
        return None

pagesText = extract_text_from_pdf("PressmanBook 7th edition.pdf")

"""# Doing cleaning for each page"""

CleanedPageText = []
for i in range(len(pagesText)):
  CleanedPageText.append(clean_text(pagesText[i]))

"""# Counting word frequency for each page"""

word_freq_array_for_pages = []
for i in range(len(CleanedPageText)):
  word_freq_array_for_pages.append(frequency_count(CleanedPageText[i]))

"""# Plotting Each page using bar graph"""

import matplotlib.pyplot as plt

def plot_word_frequencies(word_counts, chaptercount):

   if len(word_counts)>0:
     # Sort the dictionary by frequency in descending order
    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

    # Get the top 'num_words' words and their frequencies
    top_words = sorted_word_counts[:len(word_counts)]

    # Extract words and frequencies
    words, frequencies = zip(*top_words)

    # Create a vertical bar graph
    plt.figure(figsize=(20, 8))
    plt.bar(words, frequencies, color='skyblue')

    # Add titles and labels
    plt.title(f"Top {len(word_counts)} Word Frequencies from page {chaptercount}")
    plt.xlabel('Words')
    plt.ylabel('Frequency')

    # Rotate x-axis labels for readability
    plt.xticks(rotation=45, ha='right')

    # Adjust layout
    plt.tight_layout()
    plt.show()

"""# Plotting word frequency for 200 pages"""

for i in range(201):
  plot_word_frequencies(word_freq_array_for_pages[i],i)

"""# Taking random 100 pages"""

import random

def select_random_elements_from_array(input_array, n):

    if not isinstance(input_array, list):
        raise ValueError("The input must be a list.")
    if n > len(input_array):
        raise ValueError("Number of elements to select cannot exceed the number of items in the array.")

    return random.sample(input_array, n)

randomPages = select_random_elements_from_array(word_freq_array_for_pages,100)

"""# Making Adjcency matrix for random pages"""

adjacency_matrix_for_pages = create_adjacency_matrix(randomPages)
adjacency_matrix_for_pages

"""# Making graph for this"""

visualize_graph_from_adj_matrix(np.array(adjacency_matrix_for_pages))

"""# Calculating clustering cofficient for pages"""

avg_cc_page, node_cc_page = calculate_clustering_coefficient(adjacency_matrix_for_pages)
print(f"Average Clustering Coefficient: {avg_cc_page}")
print("Clustering Coefficients for each node:")
for node, cc in node_cc_page.items():
    print(f"Node {node+1}: {cc}")

"""# Extracting paragraph from book"""

import fitz  # PyMuPDF

def extract_paragraphs_from_pdf(pdf_path):

    try:
        # Open the PDF file
        document = fitz.open(pdf_path)

        paragraphs = []

        # Iterate through each page in the PDF
        for page_num in range(len(document)):
            page = document.load_page(page_num)

            # Extract the text from the page
            text = page.get_text("text")  # Extract text in raw format

            # Split text into paragraphs (assuming paragraphs are separated by '\n\n')
            page_paragraphs = text.split('\n\n')

            # Add the paragraphs to the list
            paragraphs.extend(page_paragraphs)

        return paragraphs

    except Exception as e:
        print(f"An error occurred: {e}")
        return []